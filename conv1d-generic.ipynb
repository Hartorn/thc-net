{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:43.941660Z",
     "start_time": "2020-05-25T20:59:43.833639Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:44.433824Z",
     "start_time": "2020-05-25T20:59:43.943368Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:45.767278Z",
     "start_time": "2020-05-25T20:59:44.435849Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    SpatialDropout1D,\n",
    "    LocallyConnected1D,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    LayerNormalization,\n",
    "    Concatenate\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow_addons.activations import mish\n",
    "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from itertools import repeat\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:45.781039Z",
     "start_time": "2020-05-25T20:59:45.769170Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_to_np_array(word, cut_length):\n",
    "    result = np.zeros(cut_length, dtype=\"uint8\")\n",
    "    for i, letter in enumerate(word[:cut_length]):\n",
    "        result[i] = ord(letter)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:45.893504Z",
     "start_time": "2020-05-25T20:59:45.782485Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_to_img(line, cut_length):\n",
    "    result = np.zeros((line.shape[0], cut_length), dtype=\"uint8\")\n",
    "    for i in range(line.shape[0]):\n",
    "        result[i] = word_to_np_array(line[i], cut_length)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.016629Z",
     "start_time": "2020-05-25T20:59:45.895831Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_parallel_numpy(map_func, iter_params, constant_params=None):\n",
    "    repeated_params = (\n",
    "        [] if constant_params is None else list(map(repeat, constant_params))\n",
    "    )\n",
    "    results = None\n",
    "    with PoolExecutor() as executor:\n",
    "        results = np.stack(\n",
    "            list(executor.map(map_func, *iter_params, *repeated_params)), axis=0\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.106802Z",
     "start_time": "2020-05-25T20:59:46.020842Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force and out.exists():\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.210035Z",
     "start_time": "2020-05-25T20:59:46.110562Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if \"loss\" in s and \"val\" not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if \"loss\" in s and \"val\" in s]\n",
    "    acc_list = [s for s in history.history.keys() if \"AUC\" in s and \"val\" not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if \"AUC\" in s and \"val\" in s]\n",
    "\n",
    "    if len(loss_list) == 0:\n",
    "        print(\"Loss is missing in history\")\n",
    "        return\n",
    "\n",
    "    ## As loss always exists\n",
    "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
    "\n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(\n",
    "            epochs,\n",
    "            history.history[l],\n",
    "            \"b\",\n",
    "            label=\"Training loss (\"\n",
    "            + str(str(format(history.history[l][-1], \".5f\")) + \")\"),\n",
    "        )\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(\n",
    "            epochs,\n",
    "            history.history[l],\n",
    "            \"g\",\n",
    "            label=\"Validation loss (\"\n",
    "            + str(str(format(history.history[l][-1], \".5f\")) + \")\"),\n",
    "        )\n",
    "\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"bank-marketing\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"y\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"portoseguro\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"target\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"census-income\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"taxable income amount\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"rl\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"target\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"open-payments\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"status\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"give-me-some-credit\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"SeriousDlqin2yrs\"\n",
    "ids = [\"Unamed\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"amazon\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"ACTION\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_name = \"santander-customer-satisfaction\"\n",
    "filename = \"train_bench.csv\"\n",
    "target = \"TARGET\"\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Path(os.getcwd()) / \"data\" / dataset_name / filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(out)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Set\" not in train.columns:\n",
    "    print(\"Building tailored column\")\n",
    "    train_valid_index, test_index = next(\n",
    "        StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED).split(\n",
    "            range(train[target].shape[0]), train[target].values\n",
    "        )\n",
    "    )\n",
    "    train_index, valid_index = next(\n",
    "        StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED).split(\n",
    "            train_valid_index, train[target].values[train_valid_index]\n",
    "        )\n",
    "    )\n",
    "    train[\"Set\"] = \"train\"\n",
    "    train[\"Set\"][valid_index] = \"valid\"\n",
    "    train[\"Set\"][test_index] = \"test\"\n",
    "    # train.to_csv((out.parent / \"train_bench.csv\").as_posix(), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ids = train.columns[train.dtypes != \"object\"][\n",
    "    (np.sum(train[train.columns[train.dtypes != \"object\"]] > 1e9) > 0)\n",
    "].tolist()\n",
    "print(len(big_ids))\n",
    "big_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[big_ids] = train[big_ids].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(\n",
    "    set(\n",
    "        train.columns[\n",
    "            (\n",
    "                (\n",
    "                    train[train[\"Set\"] != \"test\"].nunique()\n",
    "                    / train[train[\"Set\"] != \"test\"].shape[0]\n",
    "                )\n",
    "                < 0.05\n",
    "            )\n",
    "            & (train.dtypes != \"object\")\n",
    "        ].tolist()\n",
    "    )\n",
    "    - set([target])\n",
    ")\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[cat_cols] = train[cat_cols].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_cols = train.columns[train.nunique() <= 1]\n",
    "constant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique = train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = train.columns[n_unique == 2]\n",
    "bool_cols = list(set(bool_cols.tolist()) - set([target]) - set([\"Set\"]))\n",
    "bool_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_encoder = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in bool_cols:\n",
    "    enc = LabelEncoder()\n",
    "    train[col] = enc.fit_transform(train[col].values.astype(\"str\").reshape(-1))\n",
    "    bool_encoder[col] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_cols = train.columns[n_unique <= 1]\n",
    "constant_cols = list(set(constant_cols.tolist()) - set([target]) - set([\"Set\"]))\n",
    "constant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.05  #  This means we consider this a category if 1000 elt, there is at most 50 different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(nb):\n",
    "    if not np.isfinite(nb):\n",
    "        return str(nb)\n",
    "    return np.format_float_scientific(\n",
    "        nb, precision=9, unique=False, pad_left=None, exp_digits=2, sign=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_number(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = train.columns[(n_unique > 2) & (train.dtypes != \"object\")]\n",
    "num_cols.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train[num_cols] = train[num_cols].apply(np.vectorize(format_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_cols = train.columns[\n",
    "    (\n",
    "        (train.dtypes == \"object\")\n",
    "        | ((n_unique > 2) & (((n_unique / train.shape[0]) < ratio)))\n",
    "    )\n",
    "]\n",
    "cat_cols = list(set(cat_cols.tolist()) - set([target]) - set([\"Set\"]))\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other_cols = train.columns[\n",
    "    (n_unique > 2) & (train.dtypes != \"object\") & ((n_unique / train.shape[0]) >= ratio)\n",
    "]\n",
    "other_cols = list(set(other_cols.tolist()) - set([target]) - set([\"Set\"]))\n",
    "other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.566032Z",
     "start_time": "2020-05-25T20:59:46.559340Z"
    }
   },
   "outputs": [],
   "source": [
    "target_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.661768Z",
     "start_time": "2020-05-25T20:59:46.567593Z"
    }
   },
   "outputs": [],
   "source": [
    "train[target] = target_encoder.fit_transform(train[target].values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:46.745241Z",
     "start_time": "2020-05-25T20:59:46.663423Z"
    }
   },
   "outputs": [],
   "source": [
    "used_columns = list(\n",
    "    set(train.columns.tolist())\n",
    "    - set([target])\n",
    "    - set([\"Set\"])\n",
    "    - set(ids)\n",
    "    - set(bool_cols)\n",
    "    - set(constant_cols)\n",
    "    - set(num_cols)\n",
    ")\n",
    "used_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index\n",
    "test_indices = train[train.Set == \"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[num_cols] = train[num_cols].fillna(\n",
    "    train[num_cols].min() - train[num_cols].std() / 10\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train[num_cols].values[train_indices])\n",
    "train[num_cols] = scaler.transform(train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:47.405604Z",
     "start_time": "2020-05-25T20:59:46.747053Z"
    }
   },
   "outputs": [],
   "source": [
    "# train[\"Set\"] = np.random.choice(\n",
    "#     [\"train\", \"valid\"], p=[0.8, 0.2], size=(train.shape[0],)\n",
    "# )\n",
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index\n",
    "test_indices = train[train.Set == \"test\"].index\n",
    "\n",
    "X_train = np.char.strip(train[used_columns].values[train_indices].astype(\"str\"))\n",
    "X_valid = np.char.strip(train[used_columns].values[valid_indices].astype(\"str\"))\n",
    "X_test = np.char.strip(train[used_columns].values[test_indices].astype(\"str\"))\n",
    "\n",
    "X_bool_train = train[bool_cols].values[train_indices]\n",
    "X_bool_valid = train[bool_cols].values[valid_indices]\n",
    "X_bool_test = train[bool_cols].values[test_indices]\n",
    "\n",
    "X_num_train = train[num_cols].values[train_indices]\n",
    "X_num_valid = train[num_cols].values[valid_indices]\n",
    "X_num_test = train[num_cols].values[test_indices]\n",
    "\n",
    "y_train = train[target].values[train_indices]\n",
    "y_valid = train[target].values[valid_indices]\n",
    "y_test = train[target].values[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  del train_indices, valid_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:47.412417Z",
     "start_time": "2020-05-25T20:59:47.406951Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:59:47.573456Z",
     "start_time": "2020-05-25T20:59:47.413649Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "INPUT_DIM_BOOL = X_bool_train.shape[1]\n",
    "INPUT_DIM_NUM = X_num_train.shape[1]\n",
    "\n",
    "if X_train.shape[1] > 0:\n",
    "    NB_CHANNELS = np.vectorize(len)(X_train).max()\n",
    "else:\n",
    "    NB_CHANNELS = 0\n",
    "NB_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:00.007709Z",
     "start_time": "2020-05-25T20:59:47.617541Z"
    }
   },
   "outputs": [],
   "source": [
    "if INPUT_DIM > 0:\n",
    "    X_train_preproc = do_parallel_numpy(line_to_img, [X_train], [NB_CHANNELS])\n",
    "    X_valid_preproc = do_parallel_numpy(line_to_img, [X_valid], [NB_CHANNELS])\n",
    "    X_test_preproc = do_parallel_numpy(line_to_img, [X_test], [NB_CHANNELS])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_num_train_preproc = do_parallel_numpy(line_to_img, [X_num_train], [16])\n",
    "X_num_valid_preproc = do_parallel_numpy(line_to_img, [X_num_valid], [16])\n",
    "X_num_test_preproc = do_parallel_numpy(line_to_img, [X_num_test], [16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T19:22:09.609604Z",
     "start_time": "2020-05-25T19:22:09.602138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y_train_preproc = to_categorical(y_train)\n",
    "# Y_valid_preproc = to_categorical(y_valid)\n",
    "# Y_test_preproc = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:00.061348Z",
     "start_time": "2020-05-25T21:00:00.017086Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_dim_cat,\n",
    "    input_dim_bool,\n",
    "    input_dim_num,\n",
    "    nb_channels,\n",
    "    conv_dim=[],\n",
    "    lconv_dim=[],\n",
    "    lconv_num_dim=[],\n",
    "):\n",
    "    activation = \"swish\"  #  mish\n",
    "    optimizer = Lookahead(RectifiedAdam(1e-3), sync_period=6, slow_step_size=0.5)\n",
    "\n",
    "    inputs = []\n",
    "    concats = []\n",
    "\n",
    "    if input_dim_bool > 0:\n",
    "        input_bool_layer = Input(shape=(input_dim_bool,), name=\"input_bool\")\n",
    "        inputs.append(input_bool_layer)\n",
    "        concats.append(input_bool_layer)\n",
    "\n",
    "    if input_dim_num > 0:\n",
    "        input_num_layer = Input(shape=(input_dim_num,), name=\"input_num\")\n",
    "        inputs.append(input_num_layer)\n",
    "        #         x_num_layer = input_num_layer\n",
    "        x_num_layer = Reshape((input_dim_num, 1), name=\"reshape_num_input\")(\n",
    "            input_num_layer\n",
    "        )\n",
    "        for i, lconv_layer in enumerate(lconv_num_dim):\n",
    "            name = f\"block_lconv_num_{i}_\"\n",
    "            x_num_layer = LocallyConnected1D(\n",
    "                #                 kernel_initializer=\"he_uniform\",\n",
    "                filters=lconv_layer,\n",
    "                padding=\"valid\",\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                name=name + \"conv\",\n",
    "                use_bias=False,\n",
    "                activation=None,\n",
    "            )(x_num_layer)\n",
    "            x_num_layer = BatchNormalization(name=name + \"nb\")(x_num_layer)\n",
    "            temp_activation = \"tanh\" if i == 0 else activation\n",
    "            #  temp_activation = activation\n",
    "            x_num_layer = Activation(temp_activation, name=name + \"activation\")(\n",
    "                x_num_layer\n",
    "            )\n",
    "        nb_filters = lconv_num_dim[-1] if len(lconv_num_dim) > 0 else conv_dim[-1]\n",
    "        x_num_layer = Reshape((input_dim_num * nb_filters,), name=\"reshape_num_output\")(\n",
    "            x_num_layer\n",
    "        )\n",
    "\n",
    "        concats.append(x_num_layer)\n",
    "\n",
    "    if input_dim_cat > 0:\n",
    "        input_cat_layer = Input(shape=(input_dim_cat, nb_channels), name=\"input_cat\")\n",
    "        inputs.append(input_cat_layer)\n",
    "\n",
    "        x_layer = input_cat_layer\n",
    "\n",
    "        for i, lconv_layer in enumerate(lconv_dim):\n",
    "            name = f\"block_lconv_{i}_\"\n",
    "            x_layer = LocallyConnected1D(\n",
    "                #                 kernel_initializer=\"he_uniform\",\n",
    "                filters=lconv_layer,\n",
    "                padding=\"valid\",\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                name=name + \"lconv\",\n",
    "                use_bias=False,\n",
    "                activation=None,\n",
    "            )(x_layer)\n",
    "            x_layer = BatchNormalization(name=name + \"nb\")(x_layer)\n",
    "            #             temp_activation = \"sigmoid\" if i == 0 else activation\n",
    "            temp_activation = activation\n",
    "            x_layer = Activation(temp_activation, name=name + \"activation\")(x_layer)\n",
    "        nb_filters = lconv_dim[-1] if len(lconv_dim) > 0 else conv_dim[-1]\n",
    "        x_layer = Reshape((input_dim_cat * nb_filters,), name=\"reshape\")(x_layer)\n",
    "\n",
    "        concats.append(x_layer)\n",
    "\n",
    "    if len(concats) > 1:\n",
    "        concat = Concatenate()(concats)\n",
    "    else:\n",
    "        concat = concats[0]\n",
    "    #     concat = LayerNormalization()(concat)\n",
    "#     concat = Dense(128, activation=activation, name=\"head3\")(concat)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\", name=\"output\")(concat)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[output], name=\"first_model\",)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:00.979562Z",
     "start_time": "2020-05-25T21:00:00.062758Z"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    INPUT_DIM,\n",
    "    INPUT_DIM_BOOL,\n",
    "    INPUT_DIM_NUM,\n",
    "    NB_CHANNELS,\n",
    "    conv_dim=[],\n",
    "    lconv_dim=[128, 32],\n",
    "    lconv_num_dim=[64, 16],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:00.994383Z",
     "start_time": "2020-05-25T21:00:00.981768Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model,\n",
    "    # to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:01.114121Z",
     "start_time": "2020-05-25T21:00:00.997404Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = []\n",
    "input_valid = []\n",
    "input_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INPUT_DIM_BOOL > 0:\n",
    "    input_model.append(X_bool_train)\n",
    "    input_valid.append(X_bool_valid)\n",
    "    input_test.append(X_bool_test)\n",
    "if INPUT_DIM_NUM > 0:\n",
    "    #     input_model.append(X_num_train_preproc)\n",
    "    #     input_valid.append(X_num_valid_preproc)\n",
    "    #     input_test.append(X_num_test_preproc)\n",
    "    input_model.append(X_num_train)\n",
    "    input_valid.append(X_num_valid)\n",
    "    input_test.append(X_num_test)\n",
    "\n",
    "if INPUT_DIM > 0:\n",
    "    input_model.append(X_train_preproc)\n",
    "    input_valid.append(X_valid_preproc)\n",
    "    input_test.append(X_test_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:07.717722Z",
     "start_time": "2020-05-25T21:00:01.118492Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    input_model,\n",
    "    y_train.reshape(-1, 1),\n",
    "    epochs=2000,\n",
    "    batch_size=1024,\n",
    "    validation_data=(input_valid, y_valid.reshape(-1, 1),),\n",
    "    verbose=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:07.719810Z",
     "start_time": "2020-05-25T20:59:43.864Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:07.720685Z",
     "start_time": "2020-05-25T20:59:43.865Z"
    }
   },
   "outputs": [],
   "source": [
    "model_auc = roc_auc_score(\n",
    "    y_true=y_valid, y_score=model.predict(input_valid).reshape(-1),\n",
    ")\n",
    "model_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM : 0.7847761386793823\n",
    "# Census : 0.9461137700867462\n",
    "# give me some credit : 0.8584216818313924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:00:07.722364Z",
     "start_time": "2020-05-25T20:59:43.867Z"
    }
   },
   "outputs": [],
   "source": [
    "model_auc = roc_auc_score(y_true=y_test, y_score=model.predict(input_test).reshape(-1),)\n",
    "model_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BM : 0.8091600443913225\n",
    "# Census : 0.9467201048401863\n",
    "# give me some credit : 0.8599316528022821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version V3 => number are fillna, and activation is tanh instead of mish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VERSION\n",
    "# Bank marketing\n",
    "# valid 0.7974101623084582 test 0.8133980360868731     conv_dim=[],    lconv_dim=[128, 64, 32],    lconv_num_dim=[64, 32, 16], patience 20\n",
    "# RL\n",
    "# valid 0.9334586431074957 test 0.9331843177543191     conv_dim=[],    lconv_dim=[128, 64, 32],    lconv_num_dim=[64, 32, 16], patience 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census example\n",
    "# valid 0.9282381974389771 test 0.9262939626480025 conv_dim=[64], lconv_dim=[128, 64, 32] patience 50\n",
    "\n",
    "# RL\n",
    "# valid 0.9363136991351992 test 0.9431532242454923 conv_dim=[64], lconv_dim=[128, 64, 32] patience 50\n",
    "\n",
    "# Open payments\n",
    "# valid 0.9395366568006073 test 0.9370193221838594 conv_dim=[64], lconv_dim=[128, 64, 32] patience 50\n",
    "\n",
    "# give-me-some-credit\n",
    "# valid  test  conv_dim=[64], lconv_dim=[128, 64, 32] patience 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NB = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INPUT_DIM > 0:\n",
    "    sample = X_valid_preproc[:SAMPLE_NB]\n",
    "if INPUT_DIM_BOOL > 0:\n",
    "    bool_sample = X_bool_valid[:SAMPLE_NB]\n",
    "if INPUT_DIM_NUM > 0:\n",
    "    num_sample = X_num_valid[:SAMPLE_NB]\n",
    "input_sample = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INPUT_DIM_BOOL > 0:\n",
    "    input_sample.append(bool_sample)\n",
    "if INPUT_DIM_NUM > 0:\n",
    "    input_sample.append(num_sample)\n",
    "if INPUT_DIM > 0:\n",
    "    input_sample.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = model.predict(input_sample)\n",
    "model_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-3].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(\n",
    "    inputs=[model.inputs],\n",
    "    outputs=[model.output, model.layers[-2].output, model.layers[-3].output],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_pred, feature_cat_inter, feature_num_inter = new_model.predict(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(model_pred == new_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_inter.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_inter.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_sample_zero = np.zeros(num_sample.shape)\n",
    "bool_sample_zero = np.zeros(bool_sample.shape)\n",
    "\n",
    "for idx in range(sample.shape[1]):\n",
    "    real_idx = bool_sample.shape[1] + num_sample.shape[1] + idx\n",
    "    sample_one_col = np.zeros(sample.shape)\n",
    "\n",
    "    sample_one_col[:, idx] = sample[:, idx]\n",
    "    _, feature_inter_one_col, _ = new_model.predict(\n",
    "        (bool_sample_zero, num_sample_zero, sample_one_col)\n",
    "    )\n",
    "    #     for elt in range(feature_inter_one_col.shape[2]):\n",
    "    #         print(elt)\n",
    "    #         print(feature_inter_one_col[:, idx, :][elt])\n",
    "    #         print(feature_cat_inter[:, idx, :][elt])\n",
    "    #     print(np.sum(feature_inter_one_col[:, idx, :] - feature_cat_inter[:, idx, :]))\n",
    "    assert np.all(feature_inter_one_col[:, idx, :] == feature_cat_inter[:, idx, :])\n",
    "#     assert np.sum(feature_inter_one_col - feature_cat_inter) > 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = 0\n",
    "real_idx = bool_sample.shape[1] + num_sample.shape[1] + idx\n",
    "sample_one_col = np.zeros(sample.shape)\n",
    "\n",
    "sample_one_col[:, idx] = sample[:, idx]\n",
    "_, feature_inter_one_col, _ = new_model.predict(\n",
    "    (bool_sample_zero, num_sample_zero, sample_one_col)\n",
    ")\n",
    "assert np.all(feature_inter_one_col[:, idx, :] == feature_cat_inter[:, idx, :])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_inter_one_col[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=[model.input], outputs=[model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_train_logistic = new_model.predict(input_model)\n",
    "new_valid_logistic = new_model.predict(input_valid)\n",
    "new_test_logistic = new_model.predict(input_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LogisticRegression(max_iter=300, n_jobs=-1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lreg.fit(new_train_logistic, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_valid, y_score=lreg.predict_proba(new_valid_logistic)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BM : 0.7914463179718816\n",
    "# give me some credit :0.8582349903898445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_test, y_score=lreg.predict_proba(new_test_logistic)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM : 0.8097976507760546\n",
    "# give me some credit :0.8593843699840211\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lreg_cv = LogisticRegressionCV(Cs=10, max_iter=300, n_jobs=-1, cv=5, random_state=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "lreg_cv.fit(new_train_logistic, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_valid, y_score=lreg_cv.predict_proba(new_valid_logistic)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_test, y_score=lreg_cv.predict_proba(new_test_logistic)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(verbosity=1, tree_method=\"hist\", seed=SEED,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index\n",
    "test_indices = train[train.Set == \"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_logistic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in used_columns:\n",
    "    enc = LabelEncoder()\n",
    "    train[col] = enc.fit_transform(train[col].values.astype(\"str\").reshape(-1))\n",
    "    cat_cols[col] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_concat_train = np.hstack(\n",
    "    [\n",
    "        new_train_logistic,\n",
    "        train[num_cols].values[train_indices],\n",
    "        train[used_columns].values[train_indices],\n",
    "    ]\n",
    ")\n",
    "new_concat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_concat_valid = np.hstack(\n",
    "    [\n",
    "        new_valid_logistic,\n",
    "        train[num_cols].values[valid_indices],\n",
    "        train[used_columns].values[valid_indices],\n",
    "    ]\n",
    ")\n",
    "new_concat_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_concat_test = np.hstack(\n",
    "    [\n",
    "        new_test_logistic,\n",
    "        train[num_cols].values[test_indices],\n",
    "        train[used_columns].values[test_indices],\n",
    "    ]\n",
    ")\n",
    "new_concat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(\n",
    "    new_concat_train,\n",
    "    y_train.reshape(-1),\n",
    "    eval_set=[(new_concat_valid, y_valid.reshape(-1))],\n",
    "    eval_metric=\"auc\",\n",
    "    early_stopping_rounds=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_valid, y_score=xgb.predict_proba(new_concat_valid)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BM : 0.7950377112004379\n",
    "# give me some credit :0.8608505974744193\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_test, y_score=xgb.predict_proba(new_concat_test)[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BM : 0.8175536181491652\n",
    "# give me some credit :0.8636238967612501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_valid = (\n",
    "    lreg.predict_proba(new_valid_logistic)[:, 1].reshape(-1)\n",
    "    + xgb.predict_proba(new_concat_valid)[:, 1].reshape(-1)\n",
    "    # + model.predict(input_valid).reshape(-1)\n",
    ") / 2\n",
    "stack_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_valid, y_score=stack_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM 0.7989376973709024\n",
    "# give me some credit :0.8620617468400065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_test = (\n",
    "    lreg.predict_proba(new_test_logistic)[:, 1].reshape(-1)\n",
    "    + xgb.predict_proba(new_concat_test)[:, 1].reshape(-1)\n",
    "    # + model.predict(input_test).reshape(-1)\n",
    ") / 2\n",
    "stack_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(\n",
    "    y_true=y_test, y_score=stack_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM 0.8212055857859865\n",
    "# give me some credit :0.8640850442432623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainable_model = Model(\n",
    "    inputs=[model.input], outputs=[model.output, model.layers[-3].output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, explainability = explainable_model.predict(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[-6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-6].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-7].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainable_model = Model(\n",
    "    inputs=[model.input],\n",
    "    outputs=[\n",
    "        model.output,\n",
    "        model.layers[-5].output,\n",
    "        model.layers[-7].output,\n",
    "        model.layers[-6].output,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, expl_boo, expl_num, expl_others = explainable_model.predict(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_weight = model.get_weights()[-2][:1]\n",
    "bool_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weight = model.get_weights()[-2][1 : 9 * 16 + 1].reshape(-1, 16)\n",
    "num_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_weight = model.get_weights()[-2][9 * 16 + 1 :].reshape(-1, 32)\n",
    "others_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_boo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_num[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_others[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 9 * 16 + 9 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pred = (\n",
    "    expl_boo[0].reshape(-1) * bool_weight.reshape(-1)\n",
    "    + (expl_num[0].reshape(-1) * num_weight.reshape(-1)).sum()\n",
    "    + (expl_others[0].reshape(-1) * others_weight.reshape(-1)).sum()\n",
    ") + model.layers[-1].get_weights()[1]\n",
    "manual_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(sigmoid(manual_pred).numpy(), preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(model.get_weights()[-2] == model.layers[-1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_num[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_explain = np.hstack(\n",
    "    [\n",
    "        (expl_boo * bool_weight).sum(axis=-1).reshape(-1, 1),\n",
    "        (expl_num * num_weight).sum(-1),\n",
    "        (expl_others * others_weight).sum(axis=-1),\n",
    "    ]\n",
    ")\n",
    "features_explain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(\n",
    "    sigmoid(features_explain.sum(axis=1) + model.layers[-1].get_weights()[1]).numpy(),\n",
    "    preds.reshape(-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def explain_plot(importances, columns):\n",
    "    # objects = ('Python', 'C++', 'Java', 'Perl', 'Scala', 'Lisp')\n",
    "    y_pos = np.arange(importances.shape[0])\n",
    "    indexes = np.argsort(importances)\n",
    "    performance = importances[indexes]\n",
    "\n",
    "    plt.barh(y_pos, performance, align=\"center\", alpha=0.5)\n",
    "    plt.yticks(y_pos, columns[indexes])\n",
    "    # plt.xlabel('Usage')\n",
    "    plt.title(\"Feature importance\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = np.array(bool_cols + num_cols.tolist() + used_columns)\n",
    "all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_plot(np.abs(features_explain).sum(axis=0), all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_plot(features_explain.sum(axis=0), all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_plot(features_explain[10], all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_plot(features_explain[0], all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivate(x):\n",
    "    return np.exp(-x) / (np.exp(-x) + 1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_derivate(features_explain[10]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(features_explain[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_explain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_plot(features_explain[0], all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for explanation in features_explain[:10]:\n",
    "    explain_plot(explanation, all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
