{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (7.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nb_black in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (1.0.7)\n",
      "Requirement already satisfied: ipython in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from nb_black) (7.13.0)\n",
      "Requirement already satisfied: black>='19.3'; python_version >= \"3.6\" in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from nb_black) (19.10b0)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (41.2.0)\n",
      "Requirement already satisfied: backcall in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (3.0.4)\n",
      "Requirement already satisfied: decorator in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.16.0)\n",
      "Requirement already satisfied: pickleshare in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.8.0)\n",
      "Requirement already satisfied: pygments in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from ipython->nb_black) (4.3.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (0.8.0)\n",
      "Requirement already satisfied: toml>=0.9.4 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (0.10.0)\n",
      "Requirement already satisfied: click>=6.5 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (7.1.1)\n",
      "Requirement already satisfied: appdirs in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (1.4.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (19.3.0)\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (1.4.1)\n",
      "Requirement already satisfied: regex in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from black>='19.3'; python_version >= \"3.6\"->nb_black) (2020.4.4)\n",
      "Requirement already satisfied: wcwidth in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nb_black) (0.1.9)\n",
      "Requirement already satisfied: parso>=0.5.2 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from jedi>=0.10->ipython->nb_black) (0.6.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->nb_black) (0.6.0)\n",
      "Requirement already satisfied: six in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (1.14.0)\n",
      "Requirement already satisfied: ipython-genutils in ./.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (0.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Pre requisites for this notebook\\n!pip install Pillow\\n!pip install nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Pre requisites for this notebook\\n!pip install Pillow\\n!pip install nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre requisites for this notebook\n",
    "!pip install Pillow\n",
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nfrom requests import get\\nfrom pathlib import Path\\nimport gc\\nimport tensorflow as tf\\nfrom concurrent.futures import ProcessPoolExecutor as PoolExecutor\\n\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom tensorflow_addons.optimizers import RectifiedAdam, Lookahead\\nfrom tensorflow_addons.activations import mish\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.metrics import roc_auc_score\\n\\nnp.random.seed(0)\\n\\nfrom PIL import Image, ImageDraw, ImageFont\\nfrom matplotlib.pyplot import imshow\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"import os\\nfrom requests import get\\nfrom pathlib import Path\\nimport gc\\nimport tensorflow as tf\\nfrom concurrent.futures import ProcessPoolExecutor as PoolExecutor\\n\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom tensorflow_addons.optimizers import RectifiedAdam, Lookahead\\nfrom tensorflow_addons.activations import mish\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.metrics import roc_auc_score\\n\\nnp.random.seed(0)\\n\\nfrom PIL import Image, ImageDraw, ImageFont\\nfrom matplotlib.pyplot import imshow\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "from tensorflow_addons.activations import mish\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def download(url, out, force=False, verify=True):\\n    out.parent.mkdir(parents=True, exist_ok=True)\\n    if force:\\n        print(f\\\"Removing file at {str(out)}\\\")\\n        out.unlink()\\n\\n    if out.exists():\\n        print(\\\"File already exists.\\\")\\n        return\\n    print(f\\\"Downloading {url} at {str(out)} ...\\\")\\n    # open in binary mode\\n    with out.open(mode=\\\"wb\\\") as file:\\n        # get request\\n        response = get(url, verify=verify)\\n        for chunk in response.iter_content(100000):\\n            # write to file\\n            file.write(chunk)\";\n",
       "                var nbb_formatted_code = \"def download(url, out, force=False, verify=True):\\n    out.parent.mkdir(parents=True, exist_ok=True)\\n    if force:\\n        print(f\\\"Removing file at {str(out)}\\\")\\n        out.unlink()\\n\\n    if out.exists():\\n        print(\\\"File already exists.\\\")\\n        return\\n    print(f\\\"Downloading {url} at {str(out)} ...\\\")\\n    # open in binary mode\\n    with out.open(mode=\\\"wb\\\") as file:\\n        # get request\\n        response = get(url, verify=verify)\\n        for chunk in response.iter_content(100000):\\n            # write to file\\n            file.write(chunk)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force:\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# url = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\\\"\\n# url_test = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\\\"\\nfont_url = \\\"https://ff.static.1001fonts.net/r/o/roboto-condensed.regular.ttf\\\"\\n\\ndataset_name = \\\"bank-marketing\\\"\\nout = Path(os.getcwd()) / f\\\"data/{dataset_name}/train_bench.csv\\\"\\n# out_test = Path(os.getcwd()) / f\\\"data/{dataset_name}_test.csv\\\"\\nout_font = Path(os.getcwd()) / f\\\"RobotoCondensed-Regular.ttf\\\"\\n\\n# download(url, out)\\n# download(url_test, out_test)\\ndownload(font_url, out_font)\";\n",
       "                var nbb_formatted_code = \"# url = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\\\"\\n# url_test = \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\\\"\\nfont_url = \\\"https://ff.static.1001fonts.net/r/o/roboto-condensed.regular.ttf\\\"\\n\\ndataset_name = \\\"bank-marketing\\\"\\nout = Path(os.getcwd()) / f\\\"data/{dataset_name}/train_bench.csv\\\"\\n# out_test = Path(os.getcwd()) / f\\\"data/{dataset_name}_test.csv\\\"\\nout_font = Path(os.getcwd()) / f\\\"RobotoCondensed-Regular.ttf\\\"\\n\\n# download(url, out)\\n# download(url_test, out_test)\\ndownload(font_url, out_font)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "# url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "font_url = \"https://ff.static.1001fonts.net/r/o/roboto-condensed.regular.ttf\"\n",
    "\n",
    "dataset_name = \"bank-marketing\"\n",
    "out = Path(os.getcwd()) / f\"data/{dataset_name}/train_bench.csv\"\n",
    "# out_test = Path(os.getcwd()) / f\"data/{dataset_name}_test.csv\"\n",
    "out_font = Path(os.getcwd()) / f\"RobotoCondensed-Regular.ttf\"\n",
    "\n",
    "# download(url, out)\n",
    "# download(url_test, out_test)\n",
    "download(font_url, out_font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank-marketing as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 21)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"target = \\\"y\\\"\\ntrain = pd.read_csv(out, low_memory=False)\\nprint(train.shape)\";\n",
       "                var nbb_formatted_code = \"target = \\\"y\\\"\\ntrain = pd.read_csv(out, low_memory=False)\\nprint(train.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"y\"\n",
    "train = pd.read_csv(out, low_memory=False)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"train_indices = train[train.Set == \\\"train\\\"].index.values\\nnp.random.shuffle(train_indices)\\nvalid_indices = train[train.Set == \\\"valid\\\"].index.values\\ntest_indices = train[train.Set == \\\"test\\\"].index.values\";\n",
       "                var nbb_formatted_code = \"train_indices = train[train.Set == \\\"train\\\"].index.values\\nnp.random.shuffle(train_indices)\\nvalid_indices = train[train.Set == \\\"valid\\\"].index.values\\ntest_indices = train[train.Set == \\\"test\\\"].index.values\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_indices = train[train.Set == \"train\"].index.values\n",
    "np.random.shuffle(train_indices)\n",
    "valid_indices = train[train.Set == \"valid\"].index.values\n",
    "test_indices = train[train.Set == \"test\"].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41188, 1)\n",
      "(41188, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from sklearn import preprocessing\\n\\n# label_encoder object knows how to understand word labels.\\nlabel_encoder = preprocessing.LabelEncoder()\\n\\n# Encode labels in column 'species'.\\nY = (\\n    label_encoder.fit_transform(train[[target]].values.reshape(-1))\\n    .reshape(-1, 1)\\n    .astype(\\\"uint8\\\")\\n)\\nprint(Y.shape)\\n\\nX = train.drop(columns=[target, \\\"Set\\\"]).values.astype(\\\"str\\\")\\nprint(X.shape)\\n\\ndel train\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"from sklearn import preprocessing\\n\\n# label_encoder object knows how to understand word labels.\\nlabel_encoder = preprocessing.LabelEncoder()\\n\\n# Encode labels in column 'species'.\\nY = (\\n    label_encoder.fit_transform(train[[target]].values.reshape(-1))\\n    .reshape(-1, 1)\\n    .astype(\\\"uint8\\\")\\n)\\nprint(Y.shape)\\n\\nX = train.drop(columns=[target, \\\"Set\\\"]).values.astype(\\\"str\\\")\\nprint(X.shape)\\n\\ndel train\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "Y = (\n",
    "    label_encoder.fit_transform(train[[target]].values.reshape(-1))\n",
    "    .reshape(-1, 1)\n",
    "    .astype(\"uint8\")\n",
    ")\n",
    "print(Y.shape)\n",
    "\n",
    "X = train.drop(columns=[target, \"Set\"]).values.astype(\"str\")\n",
    "print(X.shape)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# https://he-arc.github.io/livre-python/pillow/index.html#methodes-de-dessin\\n# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\\n# https://stackoverflow.com/questions/384759/how-to-convert-a-pil-image-into-a-numpy-array\\n# line = np.array(pic, dtypes=\\\"uint8\\\")\\n# from https://arxiv.org/pdf/1902.02160.pdf page 2\";\n",
       "                var nbb_formatted_code = \"# https://he-arc.github.io/livre-python/pillow/index.html#methodes-de-dessin\\n# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\\n# https://stackoverflow.com/questions/384759/how-to-convert-a-pil-image-into-a-numpy-array\\n# line = np.array(pic, dtypes=\\\"uint8\\\")\\n# from https://arxiv.org/pdf/1902.02160.pdf page 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://he-arc.github.io/livre-python/pillow/index.html#methodes-de-dessin\n",
    "# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n",
    "# https://stackoverflow.com/questions/384759/how-to-convert-a-pil-image-into-a-numpy-array\n",
    "# line = np.array(pic, dtypes=\"uint8\")\n",
    "# from https://arxiv.org/pdf/1902.02160.pdf page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"WHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\n\\n\\ndef word_to_square_image(text, size, cut_length=None):\\n\\n    truncated = text[:cut_length] if cut_length is not None else text\\n    max_x = np.ceil(np.sqrt(len(truncated))).astype(\\\"int\\\")\\n    character_size = np.floor(size / max_x).astype(\\\"int\\\")\\n    padding = np.floor((size - (max_x * character_size)) / 2).astype(\\\"int\\\")\\n    # Do we need pt to px conversion ? Seems like not\\n    # font_size =  int(np.floor(character_size*0.75))\\n    font_size = character_size\\n\\n    fnt = ImageFont.truetype(out_font.as_posix(), font_size)\\n    image = Image.new(\\\"RGB\\\", (size, size), BLACK)\\n    # Obtention du contexte graphique\\n    draw = ImageDraw.Draw(image)\\n    x = 0\\n    y = 0\\n    for letter in text:\\n        draw.text(\\n            (padding + x * character_size, padding + y * character_size),\\n            letter,\\n            font=fnt,\\n            fill=WHITE,\\n        )\\n        if x + 1 < max_x:\\n            x += 1\\n        else:\\n            y += 1\\n            x = 0\\n    return np.array(image)\";\n",
       "                var nbb_formatted_code = \"WHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\n\\n\\ndef word_to_square_image(text, size, cut_length=None):\\n\\n    truncated = text[:cut_length] if cut_length is not None else text\\n    max_x = np.ceil(np.sqrt(len(truncated))).astype(\\\"int\\\")\\n    character_size = np.floor(size / max_x).astype(\\\"int\\\")\\n    padding = np.floor((size - (max_x * character_size)) / 2).astype(\\\"int\\\")\\n    # Do we need pt to px conversion ? Seems like not\\n    # font_size =  int(np.floor(character_size*0.75))\\n    font_size = character_size\\n\\n    fnt = ImageFont.truetype(out_font.as_posix(), font_size)\\n    image = Image.new(\\\"RGB\\\", (size, size), BLACK)\\n    # Obtention du contexte graphique\\n    draw = ImageDraw.Draw(image)\\n    x = 0\\n    y = 0\\n    for letter in text:\\n        draw.text(\\n            (padding + x * character_size, padding + y * character_size),\\n            letter,\\n            font=fnt,\\n            fill=WHITE,\\n        )\\n        if x + 1 < max_x:\\n            x += 1\\n        else:\\n            y += 1\\n            x = 0\\n    return np.array(image)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "\n",
    "def word_to_square_image(text, size, cut_length=None):\n",
    "\n",
    "    truncated = text[:cut_length] if cut_length is not None else text\n",
    "    max_x = np.ceil(np.sqrt(len(truncated))).astype(\"int\")\n",
    "    character_size = np.floor(size / max_x).astype(\"int\")\n",
    "    padding = np.floor((size - (max_x * character_size)) / 2).astype(\"int\")\n",
    "    # Do we need pt to px conversion ? Seems like not\n",
    "    # font_size =  int(np.floor(character_size*0.75))\n",
    "    font_size = character_size\n",
    "\n",
    "    fnt = ImageFont.truetype(out_font.as_posix(), font_size)\n",
    "    image = Image.new(\"RGB\", (size, size), BLACK)\n",
    "    # Obtention du contexte graphique\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for letter in text:\n",
    "        draw.text(\n",
    "            (padding + x * character_size, padding + y * character_size),\n",
    "            letter,\n",
    "            font=fnt,\n",
    "            fill=WHITE,\n",
    "        )\n",
    "        if x + 1 < max_x:\n",
    "            x += 1\n",
    "        else:\n",
    "            y += 1\n",
    "            x = 0\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def text_to_square_image(features, image_size=299, cut_length=None):\\n    square_nb = np.ceil(np.sqrt(len(features))).astype(\\\"int\\\")\\n    word_size = np.floor(image_size / square_nb).astype(\\\"int\\\")\\n    max_features = len(features)\\n    padding = np.floor((image_size - square_nb * word_size) / 2).astype(\\\"int\\\")\\n    result_image = np.zeros((image_size, image_size, 3), dtype=\\\"uint8\\\")\\n    results = []\\n    i_feature = 0\\n    for x in range(0, square_nb):\\n        if i_feature is None:\\n            break\\n        for y in range(0, square_nb):\\n            i_feature = x * (square_nb) + y\\n            if i_feature >= max_features:\\n                i_feature = None\\n                break\\n            x_pos = x * word_size + padding\\n            y_pos = y * word_size + padding\\n            result_image[\\n                x_pos : x_pos + word_size, y_pos : y_pos + word_size\\n            ] = word_to_square_image(\\n                features[i_feature], size=word_size, cut_length=cut_length\\n            )\\n    return result_image\";\n",
       "                var nbb_formatted_code = \"def text_to_square_image(features, image_size=299, cut_length=None):\\n    square_nb = np.ceil(np.sqrt(len(features))).astype(\\\"int\\\")\\n    word_size = np.floor(image_size / square_nb).astype(\\\"int\\\")\\n    max_features = len(features)\\n    padding = np.floor((image_size - square_nb * word_size) / 2).astype(\\\"int\\\")\\n    result_image = np.zeros((image_size, image_size, 3), dtype=\\\"uint8\\\")\\n    results = []\\n    i_feature = 0\\n    for x in range(0, square_nb):\\n        if i_feature is None:\\n            break\\n        for y in range(0, square_nb):\\n            i_feature = x * (square_nb) + y\\n            if i_feature >= max_features:\\n                i_feature = None\\n                break\\n            x_pos = x * word_size + padding\\n            y_pos = y * word_size + padding\\n            result_image[\\n                x_pos : x_pos + word_size, y_pos : y_pos + word_size\\n            ] = word_to_square_image(\\n                features[i_feature], size=word_size, cut_length=cut_length\\n            )\\n    return result_image\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_to_square_image(features, image_size=299, cut_length=None):\n",
    "    square_nb = np.ceil(np.sqrt(len(features))).astype(\"int\")\n",
    "    word_size = np.floor(image_size / square_nb).astype(\"int\")\n",
    "    max_features = len(features)\n",
    "    padding = np.floor((image_size - square_nb * word_size) / 2).astype(\"int\")\n",
    "    result_image = np.zeros((image_size, image_size, 3), dtype=\"uint8\")\n",
    "    results = []\n",
    "    i_feature = 0\n",
    "    for x in range(0, square_nb):\n",
    "        if i_feature is None:\n",
    "            break\n",
    "        for y in range(0, square_nb):\n",
    "            i_feature = x * (square_nb) + y\n",
    "            if i_feature >= max_features:\n",
    "                i_feature = None\n",
    "                break\n",
    "            x_pos = x * word_size + padding\n",
    "            y_pos = y * word_size + padding\n",
    "            result_image[\n",
    "                x_pos : x_pos + word_size, y_pos : y_pos + word_size\n",
    "            ] = word_to_square_image(\n",
    "                features[i_feature], size=word_size, cut_length=cut_length\n",
    "            )\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"def preprocess_data(data, map_func):\\n    preprocessed_df = None\\n    with PoolExecutor() as executor:\\n        preprocessed_df = np.stack(list(executor.map(map_func, data)), axis=0)\\n    print(preprocessed_df.shape)\\n    print(preprocessed_df.nbytes / (1024 * 1024))  # Memory size in RAM\\n    gc.collect()\\n    return preprocessed_df\";\n",
       "                var nbb_formatted_code = \"def preprocess_data(data, map_func):\\n    preprocessed_df = None\\n    with PoolExecutor() as executor:\\n        preprocessed_df = np.stack(list(executor.map(map_func, data)), axis=0)\\n    print(preprocessed_df.shape)\\n    print(preprocessed_df.nbytes / (1024 * 1024))  # Memory size in RAM\\n    gc.collect()\\n    return preprocessed_df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(data, map_func):\n",
    "    preprocessed_df = None\n",
    "    with PoolExecutor() as executor:\n",
    "        preprocessed_df = np.stack(list(executor.map(map_func, data)), axis=0)\n",
    "    print(preprocessed_df.shape)\n",
    "    print(preprocessed_df.nbytes / (1024 * 1024))  # Memory size in RAM\n",
    "    gc.collect()\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"def fixed_text_to_square_image(values):\\n    return text_to_square_image(values, image_size=96, cut_length=None)\";\n",
       "                var nbb_formatted_code = \"def fixed_text_to_square_image(values):\\n    return text_to_square_image(values, image_size=96, cut_length=None)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fixed_text_to_square_image(values):\n",
    "    return text_to_square_image(values, image_size=96, cut_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"from tensorflow.keras.utils import to_categorical\\nfrom concurrent.futures import ProcessPoolExecutor as PoolExecutor\\n\\n\\nclass TabularToImagesDataset:\\n    def __init__(self, values, target, func, prefetch=1024):\\n        self.values = values\\n        self.target = to_categorical(target.reshape(-1))\\n        assert target.shape[0] == self.values.shape[0]\\n        self.current = -1\\n        self.max_prefetch = -1\\n        self.prefetch_nb = prefetch\\n        self.func = func\\n        self.ready = None\\n\\n    def __iter__(self):\\n        # print(\\\"Calling __iter__\\\")\\n        self.current = -1\\n        self.max_prefetch = -1\\n        return self\\n\\n    def __call__(self):\\n        # print(\\\"Calling __call__\\\")\\n        self.current = -1\\n        self.max_prefetch = -1\\n        return self\\n\\n    def __next__(self):\\n        return self.next()\\n\\n    def __len__(self):\\n        return self.values.shape[0]\\n\\n    def prefetch(self):\\n        if self.ready is not None and self.ready.shape[0] == self.values.shape[0]:\\n            return\\n        # print(\\\"HERE\\\")\\n        self.max_prefetch = min(self.current + self.prefetch_nb, self.values.shape[0])\\n        # if self.current == self.max_prefetch:\\n\\n        with PoolExecutor() as executor:\\n            self.ready = np.stack(\\n                list(\\n                    executor.map(\\n                        self.func, self.values[self.current : self.max_prefetch]\\n                    )\\n                ),\\n                axis=0,\\n            )\\n        return\\n\\n    def next(self):\\n        self.current += 1\\n        # print(self.current)\\n        if self.current >= self.values.shape[0]:\\n            raise StopIteration()\\n        # self.current = 0\\n        # self.max_prefetch = -1\\n        if self.current >= self.max_prefetch:\\n            # print(\\\"Will prefetch\\\")\\n            self.prefetch()\\n            # print(self.ready.shape)\\n        return (\\n            self.ready[self.current % self.prefetch_nb],\\n            # text_to_square_image(self.values[self.current]),\\n            self.target[self.current],\\n        )\";\n",
       "                var nbb_formatted_code = \"from tensorflow.keras.utils import to_categorical\\nfrom concurrent.futures import ProcessPoolExecutor as PoolExecutor\\n\\n\\nclass TabularToImagesDataset:\\n    def __init__(self, values, target, func, prefetch=1024):\\n        self.values = values\\n        self.target = to_categorical(target.reshape(-1))\\n        assert target.shape[0] == self.values.shape[0]\\n        self.current = -1\\n        self.max_prefetch = -1\\n        self.prefetch_nb = prefetch\\n        self.func = func\\n        self.ready = None\\n\\n    def __iter__(self):\\n        # print(\\\"Calling __iter__\\\")\\n        self.current = -1\\n        self.max_prefetch = -1\\n        return self\\n\\n    def __call__(self):\\n        # print(\\\"Calling __call__\\\")\\n        self.current = -1\\n        self.max_prefetch = -1\\n        return self\\n\\n    def __next__(self):\\n        return self.next()\\n\\n    def __len__(self):\\n        return self.values.shape[0]\\n\\n    def prefetch(self):\\n        if self.ready is not None and self.ready.shape[0] == self.values.shape[0]:\\n            return\\n        # print(\\\"HERE\\\")\\n        self.max_prefetch = min(self.current + self.prefetch_nb, self.values.shape[0])\\n        # if self.current == self.max_prefetch:\\n\\n        with PoolExecutor() as executor:\\n            self.ready = np.stack(\\n                list(\\n                    executor.map(\\n                        self.func, self.values[self.current : self.max_prefetch]\\n                    )\\n                ),\\n                axis=0,\\n            )\\n        return\\n\\n    def next(self):\\n        self.current += 1\\n        # print(self.current)\\n        if self.current >= self.values.shape[0]:\\n            raise StopIteration()\\n        # self.current = 0\\n        # self.max_prefetch = -1\\n        if self.current >= self.max_prefetch:\\n            # print(\\\"Will prefetch\\\")\\n            self.prefetch()\\n            # print(self.ready.shape)\\n        return (\\n            self.ready[self.current % self.prefetch_nb],\\n            # text_to_square_image(self.values[self.current]),\\n            self.target[self.current],\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "\n",
    "class TabularToImagesDataset:\n",
    "    def __init__(self, values, target, func, prefetch=1024):\n",
    "        self.values = values\n",
    "        self.target = to_categorical(target.reshape(-1))\n",
    "        assert target.shape[0] == self.values.shape[0]\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        self.prefetch_nb = prefetch\n",
    "        self.func = func\n",
    "        self.ready = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # print(\"Calling __iter__\")\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        return self\n",
    "\n",
    "    def __call__(self):\n",
    "        # print(\"Calling __call__\")\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.values.shape[0]\n",
    "\n",
    "    def prefetch(self):\n",
    "        if self.ready is not None and self.ready.shape[0] == self.values.shape[0]:\n",
    "            return\n",
    "        # print(\"HERE\")\n",
    "        self.max_prefetch = min(self.current + self.prefetch_nb, self.values.shape[0])\n",
    "        # if self.current == self.max_prefetch:\n",
    "\n",
    "        with PoolExecutor() as executor:\n",
    "            self.ready = np.stack(\n",
    "                list(\n",
    "                    executor.map(\n",
    "                        self.func, self.values[self.current : self.max_prefetch]\n",
    "                    )\n",
    "                ),\n",
    "                axis=0,\n",
    "            )\n",
    "        return\n",
    "\n",
    "    def next(self):\n",
    "        self.current += 1\n",
    "        # print(self.current)\n",
    "        if self.current >= self.values.shape[0]:\n",
    "            raise StopIteration()\n",
    "        # self.current = 0\n",
    "        # self.max_prefetch = -1\n",
    "        if self.current >= self.max_prefetch:\n",
    "            # print(\"Will prefetch\")\n",
    "            self.prefetch()\n",
    "            # print(self.ready.shape)\n",
    "        return (\n",
    "            self.ready[self.current % self.prefetch_nb],\n",
    "            # text_to_square_image(self.values[self.current]),\n",
    "            self.target[self.current],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"def build_tf_dataset(X_values, Y_values, image_size, fix_func, prefetch, batch_size):\\n    gen = TabularToImagesDataset(\\n        X_values, Y_values, fix_func, prefetch=prefetch * batch_size,\\n    )\\n    if prefetch * batch_size > X_values.shape[0]:\\n        prefetch = np.ceil(X_values.shape[0] / batch_size).astype(\\\"int\\\")\\n\\n    dataset = tf.data.Dataset.from_generator(\\n        gen,\\n        (tf.uint8, tf.uint8),\\n        (tf.TensorShape([image_size, image_size, 3]), tf.TensorShape([2])),\\n    )\\n\\n    dataset = dataset.repeat().batch(batch_size)\\n    return dataset.prefetch(prefetch)\";\n",
       "                var nbb_formatted_code = \"def build_tf_dataset(X_values, Y_values, image_size, fix_func, prefetch, batch_size):\\n    gen = TabularToImagesDataset(\\n        X_values, Y_values, fix_func, prefetch=prefetch * batch_size,\\n    )\\n    if prefetch * batch_size > X_values.shape[0]:\\n        prefetch = np.ceil(X_values.shape[0] / batch_size).astype(\\\"int\\\")\\n\\n    dataset = tf.data.Dataset.from_generator(\\n        gen,\\n        (tf.uint8, tf.uint8),\\n        (tf.TensorShape([image_size, image_size, 3]), tf.TensorShape([2])),\\n    )\\n\\n    dataset = dataset.repeat().batch(batch_size)\\n    return dataset.prefetch(prefetch)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_tf_dataset(X_values, Y_values, image_size, fix_func, prefetch, batch_size):\n",
    "    gen = TabularToImagesDataset(\n",
    "        X_values, Y_values, fix_func, prefetch=prefetch * batch_size,\n",
    "    )\n",
    "    if prefetch * batch_size > X_values.shape[0]:\n",
    "        prefetch = np.ceil(X_values.shape[0] / batch_size).astype(\"int\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        (tf.uint8, tf.uint8),\n",
    "        (tf.TensorShape([image_size, image_size, 3]), tf.TensorShape([2])),\n",
    "    )\n",
    "\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"epochs_1 = 2\\nepochs_2 = 200\\nimage_size = 96\\ncut_length = None\\nBATCH_SIZE = 64\\nPREFETCH = 10000  # 40\\n# SHUFFLE_BUFFER_SIZE = 10000\\npatience = 5\\n\\n\\ndef fixed_text_to_square_image(values):\\n    return text_to_square_image(values, image_size=image_size, cut_length=cut_length)\\n\\n\\nsteps_per_epoch = np.ceil(train_indices.shape[0] / BATCH_SIZE)\\nsteps_per_epoch_val = np.ceil(valid_indices.shape[0] / BATCH_SIZE)\\n\\ndataset = build_tf_dataset(\\n    X[train_indices],\\n    Y[train_indices],\\n    image_size,\\n    fixed_text_to_square_image,\\n    PREFETCH,\\n    BATCH_SIZE,\\n)\\n\\ndataset_valid = build_tf_dataset(\\n    X[valid_indices],\\n    Y[valid_indices],\\n    image_size,\\n    fixed_text_to_square_image,\\n    PREFETCH,\\n    BATCH_SIZE,\\n)\";\n",
       "                var nbb_formatted_code = \"epochs_1 = 2\\nepochs_2 = 200\\nimage_size = 96\\ncut_length = None\\nBATCH_SIZE = 64\\nPREFETCH = 10000  # 40\\n# SHUFFLE_BUFFER_SIZE = 10000\\npatience = 5\\n\\n\\ndef fixed_text_to_square_image(values):\\n    return text_to_square_image(values, image_size=image_size, cut_length=cut_length)\\n\\n\\nsteps_per_epoch = np.ceil(train_indices.shape[0] / BATCH_SIZE)\\nsteps_per_epoch_val = np.ceil(valid_indices.shape[0] / BATCH_SIZE)\\n\\ndataset = build_tf_dataset(\\n    X[train_indices],\\n    Y[train_indices],\\n    image_size,\\n    fixed_text_to_square_image,\\n    PREFETCH,\\n    BATCH_SIZE,\\n)\\n\\ndataset_valid = build_tf_dataset(\\n    X[valid_indices],\\n    Y[valid_indices],\\n    image_size,\\n    fixed_text_to_square_image,\\n    PREFETCH,\\n    BATCH_SIZE,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_1 = 2\n",
    "epochs_2 = 200\n",
    "image_size = 96\n",
    "cut_length = None\n",
    "BATCH_SIZE = 64\n",
    "PREFETCH = 10000  # 40\n",
    "# SHUFFLE_BUFFER_SIZE = 10000\n",
    "patience = 5\n",
    "\n",
    "\n",
    "def fixed_text_to_square_image(values):\n",
    "    return text_to_square_image(values, image_size=image_size, cut_length=cut_length)\n",
    "\n",
    "\n",
    "steps_per_epoch = np.ceil(train_indices.shape[0] / BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil(valid_indices.shape[0] / BATCH_SIZE)\n",
    "\n",
    "dataset = build_tf_dataset(\n",
    "    X[train_indices],\n",
    "    Y[train_indices],\n",
    "    image_size,\n",
    "    fixed_text_to_square_image,\n",
    "    PREFETCH,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "dataset_valid = build_tf_dataset(\n",
    "    X[valid_indices],\n",
    "    Y[valid_indices],\n",
    "    image_size,\n",
    "    fixed_text_to_square_image,\n",
    "    PREFETCH,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for image, label in dataset.as_numpy_iterator():\n",
    "    # print(label)\n",
    "    imshow(image[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"activation = mish\\noptimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\";\n",
       "                var nbb_formatted_code = \"activation = mish\\noptimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation = mish\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"from tensorflow.keras.applications.inception_v3 import InceptionV3\\nfrom tensorflow.keras.preprocessing import image\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import (\\n    Dense,\\n    GlobalAveragePooling2D,\\n    BatchNormalization,\\n    Dropout,\\n)\\nfrom tensorflow.keras import backend as K\\n\\n# create the base pre-trained model\\nbase_model = InceptionV3(weights=\\\"imagenet\\\", include_top=False)\\n\\n# add a global spatial average pooling layer\\nx = base_model.output\\nx = GlobalAveragePooling2D()(x)\\n# let's add a fully-connected layer\\nx = Dense(1024, activation=activation, kernel_initializer=\\\"he_normal\\\")(x)\\nx = Dropout(0.2)(x)\\nx = Dense(128, activation=activation, kernel_initializer=\\\"he_normal\\\")(x)\\nx = Dropout(0.2)(x)\\n\\n# and a logistic layer -- let's say we have 200 classes\\n# predictions = Dense(200, activation='softmax')(x)\\npredictions = Dense(2, activation=\\\"softmax\\\")(x)\\n\\n# this is the model we will train\\nmodel = Model(inputs=base_model.input, outputs=predictions)\\n\\n# first: train only the top layers (which were randomly initialized)\\n# i.e. freeze all convolutional InceptionV3 layers\\nfor layer in base_model.layers:\\n    layer.trainable = False\\n\\n# es.set_model(model)\\n# compile the model (should be done *after* setting layers to non-trainable)\\nmodel.compile(optimizer=optimizer, loss=\\\"binary_crossentropy\\\")  # , metrics=[\\\"AUC\\\"])\";\n",
       "                var nbb_formatted_code = \"from tensorflow.keras.applications.inception_v3 import InceptionV3\\nfrom tensorflow.keras.preprocessing import image\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import (\\n    Dense,\\n    GlobalAveragePooling2D,\\n    BatchNormalization,\\n    Dropout,\\n)\\nfrom tensorflow.keras import backend as K\\n\\n# create the base pre-trained model\\nbase_model = InceptionV3(weights=\\\"imagenet\\\", include_top=False)\\n\\n# add a global spatial average pooling layer\\nx = base_model.output\\nx = GlobalAveragePooling2D()(x)\\n# let's add a fully-connected layer\\nx = Dense(1024, activation=activation, kernel_initializer=\\\"he_normal\\\")(x)\\nx = Dropout(0.2)(x)\\nx = Dense(128, activation=activation, kernel_initializer=\\\"he_normal\\\")(x)\\nx = Dropout(0.2)(x)\\n\\n# and a logistic layer -- let's say we have 200 classes\\n# predictions = Dense(200, activation='softmax')(x)\\npredictions = Dense(2, activation=\\\"softmax\\\")(x)\\n\\n# this is the model we will train\\nmodel = Model(inputs=base_model.input, outputs=predictions)\\n\\n# first: train only the top layers (which were randomly initialized)\\n# i.e. freeze all convolutional InceptionV3 layers\\nfor layer in base_model.layers:\\n    layer.trainable = False\\n\\n# es.set_model(model)\\n# compile the model (should be done *after* setting layers to non-trainable)\\nmodel.compile(optimizer=optimizer, loss=\\\"binary_crossentropy\\\")  # , metrics=[\\\"AUC\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = Dense(200, activation='softmax')(x)\n",
    "predictions = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# es.set_model(model)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")  # , metrics=[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 515.0 steps, validate for 65.0 steps\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /work/.cache/poetry/thc-net-KQLMmzPP-py3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "515/515 [==============================] - 164s 318ms/step - loss: 0.3618 - val_loss: 1.7881\n",
      "Epoch 2/2\n",
      "515/515 [==============================] - 22s 43ms/step - loss: 0.3341 - val_loss: 1.4349\n",
      "CPU times: user 1min 33s, sys: 11.5 s, total: 1min 45s\n",
      "Wall time: 3min 6s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"%%time\\n# train the model on the new data for a few epochs\\nhistory_1 = model.fit(\\n    dataset, \\n    #callbacks=[es],\\n    epochs=epochs_1,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_data=dataset_valid,\\n    validation_steps=steps_per_epoch_val\\n)\\n# at this point, the top layers are well trained and we can start fine-tuning\\n# convolutional layers from inception V3. We will freeze the bottom N layers\\n# and train the remaining top layers.\";\n",
       "                var nbb_formatted_code = \"%%time\\n# train the model on the new data for a few epochs\\nhistory_1 = model.fit(\\n    dataset, \\n    #callbacks=[es],\\n    epochs=epochs_1,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_data=dataset_valid,\\n    validation_steps=steps_per_epoch_val\\n)\\n# at this point, the top layers are well trained and we can start fine-tuning\\n# convolutional layers from inception V3. We will freeze the bottom N layers\\n# and train the remaining top layers.\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# train the model on the new data for a few epochs\n",
    "history_1 = model.fit(\n",
    "    dataset, \n",
    "    #callbacks=[es],\n",
    "    epochs=epochs_1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"def plot_metric(history, metric):\\n    # Plot training & validation loss values\\n    plt.plot(history.history[metric])\\n    plt.plot(history.history[f\\\"val_{metric}\\\"])\\n    plt.title(f\\\"Model {metric}\\\")\\n    plt.ylabel(f\\\"{metric}\\\")\\n    plt.xlabel(\\\"Epoch\\\")\\n    plt.legend([\\\"Train\\\", \\\"Test\\\"], loc=\\\"upper left\\\")\\n    plt.show()\";\n",
       "                var nbb_formatted_code = \"def plot_metric(history, metric):\\n    # Plot training & validation loss values\\n    plt.plot(history.history[metric])\\n    plt.plot(history.history[f\\\"val_{metric}\\\"])\\n    plt.title(f\\\"Model {metric}\\\")\\n    plt.ylabel(f\\\"{metric}\\\")\\n    plt.xlabel(\\\"Epoch\\\")\\n    plt.legend([\\\"Train\\\", \\\"Test\\\"], loc=\\\"upper left\\\")\\n    plt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_metric(history, metric):\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f\"val_{metric}\"])\n",
    "    plt.title(f\"Model {metric}\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"plot_metric(history_1, \\\"loss\\\")\";\n",
       "                var nbb_formatted_code = \"plot_metric(history_1, \\\"loss\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(history_1, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# plot_metric(history_1, \\\"AUC\\\")\";\n",
       "                var nbb_formatted_code = \"# plot_metric(history_1, \\\"AUC\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_metric(history_1, \"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 3 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 3 96          conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, None, 3 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, None, 3 9216        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, None, 3 96          conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, None, None, 3 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, None, None, 6 18432       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, None, 6 192         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, None, None, 6 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 6 0           activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, None, 8 5120        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, None, 8 240         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, None, None, 8 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, None, 1 138240      activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, None, 1 576         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, None, None, 1 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 1 0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, None, 6 192         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, None, None, 6 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, None, 4 9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, None, None, 9 55296       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, None, 4 144         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, None, 9 288         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, None, None, 4 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, None, None, 9 0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, None, None, 1 0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, None, 6 76800       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, None, None, 9 82944       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, None, None, 3 6144        average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, None, 6 192         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, None, 6 192         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, None, 9 288         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, None, 3 96          conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, None, None, 6 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, None, None, 6 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, None, None, 9 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, None, 3 0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_99[0][0]              \n",
      "                                                                 activation_101[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, None, 6 192         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, None, None, 6 0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, None, None, 9 55296       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, None, 4 144         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, None, 9 288         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, None, 4 0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, None, None, 9 0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, None, None, 6 76800       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, None, None, 9 82944       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, None, None, 6 16384       average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, None, 6 192         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, None, 6 192         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, None, 9 288         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, None, 6 192         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, None, 6 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, None, None, 6 0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, None, None, 9 0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, None, 6 0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_106[0][0]             \n",
      "                                                                 activation_108[0][0]             \n",
      "                                                                 activation_111[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, None, 6 192         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, None, None, 6 0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, None, None, 9 55296       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, None, 4 144         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, None, 9 288         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, None, 4 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, None, None, 9 0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, None, None, 6 76800       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, None, None, 9 82944       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, None, None, 6 18432       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, None, 6 192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, None, 6 192         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, None, 9 288         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, None, 6 192         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, None, 6 0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, None, None, 6 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, None, None, 9 0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, None, 6 0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_113[0][0]             \n",
      "                                                                 activation_115[0][0]             \n",
      "                                                                 activation_118[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, None, 6 192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, None, 6 0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, None, None, 9 55296       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, None, 9 288         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, None, None, 9 0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, None, None, 9 82944       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, None, 3 1152        conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, None, 9 288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, None, 3 0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, None, None, 9 0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_120[0][0]             \n",
      "                                                                 activation_123[0][0]             \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, None, 1 384         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, None, None, 1 0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, None, None, 1 114688      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, None, 1 384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, None, None, 1 0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, None, None, 1 114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, None, 1 384         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, None, 1 384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, None, None, 1 0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, None, None, 1 0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, None, None, 1 114688      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, None, None, 1 114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, None, 1 384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, None, 1 384         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, None, 1 0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, None, None, 1 0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, None, None, 1 172032      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, None, None, 1 172032      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, None, 1 576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, None, 1 576         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, None, None, 1 576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, None, None, 1 576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, None, None, 1 0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, None, 1 0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, None, None, 1 0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, None, None, 1 0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_124[0][0]             \n",
      "                                                                 activation_127[0][0]             \n",
      "                                                                 activation_132[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, None, None, 1 480         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, None, None, 1 0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, None, None, 1 179200      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, None, None, 1 480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, None, None, 1 0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, None, None, 1 179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, None, None, 1 480         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, None, None, 1 480         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, None, None, 1 0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, None, None, 1 0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, None, None, 1 179200      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, None, None, 1 179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, None, None, 1 480         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, None, None, 1 480         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, None, None, 1 0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, None, None, 1 0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, None, None, 1 215040      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, None, None, 1 215040      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, None, None, 1 576         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, None, None, 1 576         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, None, None, 1 576         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, None, None, 1 576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, None, None, 1 0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, None, None, 1 0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, None, None, 1 0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, None, None, 1 0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_134[0][0]             \n",
      "                                                                 activation_137[0][0]             \n",
      "                                                                 activation_142[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, None, None, 1 480         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, None, 1 0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, None, None, 1 179200      activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, None, None, 1 480         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, None, 1 0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, None, None, 1 179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, None, None, 1 480         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, None, None, 1 480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, None, None, 1 0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, None, None, 1 0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, None, None, 1 179200      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, None, None, 1 179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, None, None, 1 480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, None, None, 1 480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, None, None, 1 0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, None, None, 1 0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, None, None, 1 215040      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, None, None, 1 215040      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, None, None, 1 576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, None, None, 1 576         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, None, None, 1 576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, None, None, 1 576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, None, None, 1 0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, None, None, 1 0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, None, None, 1 0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, None, None, 1 0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_144[0][0]             \n",
      "                                                                 activation_147[0][0]             \n",
      "                                                                 activation_152[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, None, None, 1 576         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, None, None, 1 0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, None, None, 1 258048      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, None, None, 1 576         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, None, None, 1 0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, None, None, 1 258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, None, None, 1 576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, None, None, 1 576         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, None, 1 0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, None, None, 1 0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, None, None, 1 258048      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, None, None, 1 258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, None, None, 1 576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, None, None, 1 576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, None, 1 0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, None, 1 0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, None, None, 1 258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, None, None, 1 258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, None, None, 1 576         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, None, None, 1 576         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, None, None, 1 576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, None, None, 1 576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, None, 1 0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, None, None, 1 0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, None, 1 0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, None, 1 0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_154[0][0]             \n",
      "                                                                 activation_157[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, None, None, 1 576         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, None, None, 1 0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, None, None, 1 258048      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, None, None, 1 576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, None, None, 1 0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, None, None, 1 258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, None, None, 1 576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, None, None, 1 576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, None, None, 1 0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, None, 1 0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, None, None, 3 552960      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, None, None, 1 331776      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, None, None, 3 960         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, None, None, 1 576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, None, None, 3 0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, None, 1 0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_165[0][0]             \n",
      "                                                                 activation_169[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, None, None, 4 1344        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, None, None, 4 0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, None, None, 3 1548288     activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, None, None, 3 1152        conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, None, None, 3 1152        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, None, None, 3 0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, None, None, 3 0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, None, None, 3 442368      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, None, None, 3 442368      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, None, None, 3 442368      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, None, None, 3 442368      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, None, None, 3 1152        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, None, None, 3 1152        conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, None, None, 3 1152        conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, None, None, 3 1152        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, None, None, 1 245760      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, None, None, 3 960         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, None, None, 3 0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, None, None, 3 0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, None, None, 3 0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, None, None, 3 0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, None, None, 1 576         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, None, None, 3 0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_172[0][0]             \n",
      "                                                                 activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_176[0][0]             \n",
      "                                                                 activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, None, None, 1 0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_170[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, None, None, 4 1344        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, None, None, 4 0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, None, None, 3 1548288     activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, None, None, 3 1152        conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, None, None, 3 1152        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, None, None, 3 0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, None, None, 3 0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, None, None, 3 442368      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, None, None, 3 442368      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, None, None, 3 442368      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, None, None, 3 442368      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, None, None, 3 1152        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, None, None, 3 1152        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, None, None, 3 1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, None, None, 3 1152        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, None, None, 1 393216      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, None, None, 3 960         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, None, None, 3 0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, None, None, 3 0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, None, None, 3 0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, None, None, 3 0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, None, None, 1 576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, None, None, 3 0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_181[0][0]             \n",
      "                                                                 activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 7 0           activation_185[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, None, None, 1 0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_179[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          131200      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,032,418\n",
      "Trainable params: 2,229,634\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# let's visualize layer names and layer indices to see how many layers\\n# we should freeze:\\nmodel.summary()\\n# for i, layer in enumerate(model.layers):\\n#    print(i, layer.name)\";\n",
       "                var nbb_formatted_code = \"# let's visualize layer names and layer indices to see how many layers\\n# we should freeze:\\nmodel.summary()\\n# for i, layer in enumerate(model.layers):\\n#    print(i, layer.name)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "model.summary()\n",
    "# for i, layer in enumerate(model.layers):\n",
    "#    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"# Let's unfreeze the whole model\\nfor layer in model.layers:\\n    layer.trainable = True\\n# Let's build an optimizer\\noptimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\\nes = EarlyStopping(\\n    monitor=\\\"val_loss\\\",\\n    verbose=1,\\n    mode=\\\"max\\\",\\n    patience=patience,\\n    restore_best_weights=True,\\n)\\n# We need to recompile the model for these modifications to take effect\\nes.set_model(model)\\nmodel.compile(optimizer=optimizer, loss=\\\"binary_crossentropy\\\")  # , metrics=[\\\"AUC\\\"])\";\n",
       "                var nbb_formatted_code = \"# Let's unfreeze the whole model\\nfor layer in model.layers:\\n    layer.trainable = True\\n# Let's build an optimizer\\noptimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\\nes = EarlyStopping(\\n    monitor=\\\"val_loss\\\",\\n    verbose=1,\\n    mode=\\\"max\\\",\\n    patience=patience,\\n    restore_best_weights=True,\\n)\\n# We need to recompile the model for these modifications to take effect\\nes.set_model(model)\\nmodel.compile(optimizer=optimizer, loss=\\\"binary_crossentropy\\\")  # , metrics=[\\\"AUC\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's unfreeze the whole model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "# Let's build an optimizer\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"max\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# We need to recompile the model for these modifications to take effect\n",
    "es.set_model(model)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")  # , metrics=[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 515.0 steps, validate for 65.0 steps\n",
      "Epoch 1/200\n",
      "515/515 [==============================] - 120s 233ms/step - loss: 0.3042 - val_loss: 0.2860\n",
      "Epoch 2/200\n",
      "515/515 [==============================] - 76s 148ms/step - loss: 0.2871 - val_loss: 0.2850\n",
      "Epoch 3/200\n",
      "515/515 [==============================] - 78s 151ms/step - loss: 0.2884 - val_loss: 0.2870\n",
      "Epoch 4/200\n",
      "515/515 [==============================] - 78s 151ms/step - loss: 0.2871 - val_loss: 0.6771\n",
      "Epoch 5/200\n",
      "515/515 [==============================] - 78s 151ms/step - loss: 0.2824 - val_loss: 0.2890\n",
      "Epoch 6/200\n",
      "515/515 [==============================] - 79s 154ms/step - loss: 0.2788 - val_loss: 0.2961\n",
      "Epoch 7/200\n",
      "515/515 [==============================] - 77s 149ms/step - loss: 0.2812 - val_loss: 0.3465\n",
      "Epoch 8/200\n",
      "515/515 [==============================] - 77s 149ms/step - loss: 0.2848 - val_loss: 0.4543\n",
      "Epoch 9/200\n",
      "514/515 [============================>.] - ETA: 0s - loss: 0.2840Restoring model weights from the end of the best epoch.\n",
      "515/515 [==============================] - 77s 150ms/step - loss: 0.2841 - val_loss: 0.2925\n",
      "Epoch 00009: early stopping\n",
      "CPU times: user 12min 45s, sys: 1min 5s, total: 13min 50s\n",
      "Wall time: 12min 20s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"%%time\\n# we train our model again (this time fine-tuning the top 2 inception blocks\\n# alongside the top Dense layers\\nhistory_2 = model.fit(\\n    dataset, \\n    callbacks=[es],\\n    epochs=epochs_2,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_data=dataset_valid,\\n    validation_steps=steps_per_epoch_val\\n)\";\n",
       "                var nbb_formatted_code = \"%%time\\n# we train our model again (this time fine-tuning the top 2 inception blocks\\n# alongside the top Dense layers\\nhistory_2 = model.fit(\\n    dataset, \\n    callbacks=[es],\\n    epochs=epochs_2,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_data=dataset_valid,\\n    validation_steps=steps_per_epoch_val\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history_2 = model.fit(\n",
    "    dataset, \n",
    "    callbacks=[es],\n",
    "    epochs=epochs_2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"plot_metric(history_2, \\\"loss\\\")\";\n",
       "                var nbb_formatted_code = \"plot_metric(history_2, \\\"loss\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(history_2, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"# plot_metric(history_2, \\\"AUC\\\")\";\n",
       "                var nbb_formatted_code = \"# plot_metric(history_2, \\\"AUC\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_metric(history_2, \"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119, 96, 96, 3)\n",
      "108.6064453125\n",
      "CPU times: user 2.35 s, sys: 2.05 s, total: 4.4 s\n",
      "Wall time: 10.8 s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"%%time\\nvalid_data = preprocess_data(X[valid_indices], fixed_text_to_square_image)\";\n",
       "                var nbb_formatted_code = \"%%time\\nvalid_data = preprocess_data(X[valid_indices], fixed_text_to_square_image)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "valid_data = preprocess_data(X[valid_indices], fixed_text_to_square_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7810644513503227\n",
      "CPU times: user 5.59 s, sys: 299 ms, total: 5.89 s\n",
      "Wall time: 6.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"%%time\\npreds = model.predict(valid_data)\\nprint(roc_auc_score(Y[valid_indices], preds[:, 1]))\\ndel valid_data\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"%%time\\npreds = model.predict(valid_data)\\nprint(roc_auc_score(Y[valid_indices], preds[:, 1]))\\ndel valid_data\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict(valid_data)\n",
    "print(roc_auc_score(Y[valid_indices], preds[:, 1]))\n",
    "del valid_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119, 96, 96, 3)\n",
      "108.6064453125\n",
      "CPU times: user 2.29 s, sys: 1.86 s, total: 4.14 s\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"%%time\\ntest_data = preprocess_data(X[test_indices], fixed_text_to_square_image)\";\n",
       "                var nbb_formatted_code = \"%%time\\ntest_data = preprocess_data(X[test_indices], fixed_text_to_square_image)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "test_data = preprocess_data(X[test_indices], fixed_text_to_square_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79967683214928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"preds = model.predict(test_data)\\nprint(roc_auc_score(Y[test_indices], preds[:, 1]))\\ndel test_data\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"preds = model.predict(test_data)\\nprint(roc_auc_score(Y[test_indices], preds[:, 1]))\\ndel test_data\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = model.predict(test_data)\n",
    "print(roc_auc_score(Y[test_indices], preds[:, 1]))\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"#https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d\";\n",
       "                var nbb_formatted_code = \"# https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
