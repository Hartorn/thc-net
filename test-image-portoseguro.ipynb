{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre requisites for this notebook\n",
    "!pip install Pillow\n",
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "from tensorflow_addons.activations import mish\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force:\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "# url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "font_url = \"https://ff.static.1001fonts.net/r/o/roboto-condensed.regular.ttf\"\n",
    "\n",
    "dataset_name = \"portoseguro\"\n",
    "out = Path(os.getcwd()) / f\"data/{dataset_name}/train_bench.csv\"\n",
    "# out_test = Path(os.getcwd()) / f\"data/{dataset_name}_test.csv\"\n",
    "out_font = Path(os.getcwd()) / f\"RobotoCondensed-Regular.ttf\"\n",
    "\n",
    "# download(url, out)\n",
    "# download(url_test, out_test)\n",
    "download(font_url, out_font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank-marketing as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target\"\n",
    "train = pd.read_csv(out, low_memory=False)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = train[train.Set == \"train\"].index.values\n",
    "np.random.shuffle(train_indices)\n",
    "valid_indices = train[train.Set == \"valid\"].index.values\n",
    "test_indices = train[train.Set == \"test\"].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "Y = (\n",
    "    label_encoder.fit_transform(train[[target]].values.reshape(-1))\n",
    "    .reshape(-1, 1)\n",
    "    .astype(\"uint8\")\n",
    ")\n",
    "print(Y.shape)\n",
    "\n",
    "X = train.drop(columns=[target, \"Set\"]).values  # .astype(\"str\")\n",
    "print(X.shape)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://he-arc.github.io/livre-python/pillow/index.html#methodes-de-dessin\n",
    "# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n",
    "# https://stackoverflow.com/questions/384759/how-to-convert-a-pil-image-into-a-numpy-array\n",
    "# line = np.array(pic, dtypes=\"uint8\")\n",
    "# from https://arxiv.org/pdf/1902.02160.pdf page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "\n",
    "def word_to_square_image(text, size, cut_length=None):\n",
    "\n",
    "    truncated = text[:cut_length] if cut_length is not None else text\n",
    "    max_x = np.ceil(np.sqrt(len(truncated))).astype(\"int\")\n",
    "    character_size = np.floor(size / max_x).astype(\"int\")\n",
    "    padding = np.floor((size - (max_x * character_size)) / 2).astype(\"int\")\n",
    "    # Do we need pt to px conversion ? Seems like not\n",
    "    # font_size =  int(np.floor(character_size*0.75))\n",
    "    font_size = character_size\n",
    "\n",
    "    fnt = ImageFont.truetype(out_font.as_posix(), font_size)\n",
    "    image = Image.new(\"RGB\", (size, size), BLACK)\n",
    "    # Obtention du contexte graphique\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for letter in text:\n",
    "        draw.text(\n",
    "            (padding + x * character_size, padding + y * character_size),\n",
    "            letter,\n",
    "            font=fnt,\n",
    "            fill=WHITE,\n",
    "        )\n",
    "        if x + 1 < max_x:\n",
    "            x += 1\n",
    "        else:\n",
    "            y += 1\n",
    "            x = 0\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_square_image(features, image_size=299, cut_length=None):\n",
    "    square_nb = np.ceil(np.sqrt(len(features))).astype(\"int\")\n",
    "    word_size = np.floor(image_size / square_nb).astype(\"int\")\n",
    "    max_features = len(features)\n",
    "    padding = np.floor((image_size - square_nb * word_size) / 2).astype(\"int\")\n",
    "    result_image = np.zeros((image_size, image_size, 3), dtype=\"uint8\")\n",
    "    results = []\n",
    "    i_feature = 0\n",
    "    for x in range(0, square_nb):\n",
    "        if i_feature is None:\n",
    "            break\n",
    "        for y in range(0, square_nb):\n",
    "            i_feature = x * (square_nb) + y\n",
    "            if i_feature >= max_features:\n",
    "                i_feature = None\n",
    "                break\n",
    "            x_pos = x * word_size + padding\n",
    "            y_pos = y * word_size + padding\n",
    "            result_image[\n",
    "                x_pos : x_pos + word_size, y_pos : y_pos + word_size\n",
    "            ] = word_to_square_image(\n",
    "                features[i_feature].astype(\"str\"), size=word_size, cut_length=cut_length\n",
    "            )\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, map_func):\n",
    "    preprocessed_df = None\n",
    "    with PoolExecutor() as executor:\n",
    "        preprocessed_df = np.stack(list(executor.map(map_func, data)), axis=0)\n",
    "    print(preprocessed_df.shape)\n",
    "    print(preprocessed_df.nbytes / (1024 * 1024))  # Memory size in RAM\n",
    "    gc.collect()\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_text_to_square_image(values):\n",
    "    return text_to_square_image(values, image_size=96, cut_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "\n",
    "class TabularToImagesDataset:\n",
    "    def __init__(self, values, target, func, prefetch=1024):\n",
    "        self.values = values\n",
    "        self.target = to_categorical(target.reshape(-1))\n",
    "        assert target.shape[0] == self.values.shape[0]\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        self.prefetch_nb = prefetch\n",
    "        self.func = func\n",
    "        self.ready = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # print(\"Calling __iter__\")\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        return self\n",
    "\n",
    "    def __call__(self):\n",
    "        # print(\"Calling __call__\")\n",
    "        self.current = -1\n",
    "        self.max_prefetch = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.values.shape[0]\n",
    "\n",
    "    def prefetch(self):\n",
    "        if self.ready is not None and self.ready.shape[0] == self.values.shape[0]:\n",
    "            return\n",
    "        # print(\"HERE\")\n",
    "        self.max_prefetch = min(self.current + self.prefetch_nb, self.values.shape[0])\n",
    "        # if self.current == self.max_prefetch:\n",
    "\n",
    "        with PoolExecutor() as executor:\n",
    "            self.ready = np.stack(\n",
    "                list(\n",
    "                    executor.map(\n",
    "                        self.func, self.values[self.current : self.max_prefetch]\n",
    "                    )\n",
    "                ),\n",
    "                axis=0,\n",
    "            )\n",
    "        return\n",
    "\n",
    "    def next(self):\n",
    "        self.current += 1\n",
    "        # print(self.current)\n",
    "        if self.current >= self.values.shape[0]:\n",
    "            raise StopIteration()\n",
    "        # self.current = 0\n",
    "        # self.max_prefetch = -1\n",
    "        if self.current >= self.max_prefetch:\n",
    "            # print(\"Will prefetch\")\n",
    "            self.prefetch()\n",
    "            # print(self.ready.shape)\n",
    "        return (\n",
    "            self.ready[self.current % self.prefetch_nb],\n",
    "            # text_to_square_image(self.values[self.current]),\n",
    "            self.target[self.current],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_dataset(X_values, Y_values, image_size, fix_func, prefetch, batch_size):\n",
    "    gen = TabularToImagesDataset(\n",
    "        X_values, Y_values, fix_func, prefetch=prefetch * batch_size,\n",
    "    )\n",
    "    if prefetch * batch_size > X_values.shape[0]:\n",
    "        prefetch = np.ceil(X_values.shape[0] / batch_size).astype(\"int\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        (tf.uint8, tf.uint8),\n",
    "        (tf.TensorShape([image_size, image_size, 3]), tf.TensorShape([2])),\n",
    "    )\n",
    "\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_1 = 2\n",
    "epochs_2 = 200\n",
    "image_size = 96\n",
    "cut_length = None\n",
    "BATCH_SIZE = 64\n",
    "PREFETCH = 10000  # 40\n",
    "# SHUFFLE_BUFFER_SIZE = 10000\n",
    "patience = 1\n",
    "\n",
    "\n",
    "def fixed_text_to_square_image(values):\n",
    "    return text_to_square_image(values, image_size=image_size, cut_length=cut_length)\n",
    "\n",
    "\n",
    "steps_per_epoch = np.ceil(train_indices.shape[0] / BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil(valid_indices.shape[0] / BATCH_SIZE)\n",
    "\n",
    "dataset = build_tf_dataset(\n",
    "    X[train_indices],\n",
    "    Y[train_indices],\n",
    "    image_size,\n",
    "    fixed_text_to_square_image,\n",
    "    PREFETCH,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "dataset_valid = build_tf_dataset(\n",
    "    X[valid_indices],\n",
    "    Y[valid_indices],\n",
    "    image_size,\n",
    "    fixed_text_to_square_image,\n",
    "    PREFETCH,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for image, label in dataset.as_numpy_iterator():\n",
    "    # print(label)\n",
    "    imshow(image[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = mish\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = Dense(200, activation='softmax')(x)\n",
    "predictions = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# es.set_model(model)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")  # , metrics=[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model on the new data for a few epochs\n",
    "history_1 = model.fit(\n",
    "    dataset, \n",
    "    #callbacks=[es],\n",
    "    epochs=epochs_1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history, metric):\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f\"val_{metric}\"])\n",
    "    plt.title(f\"Model {metric}\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history_1, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metric(history_1, \"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "model.summary()\n",
    "# for i, layer in enumerate(model.layers):\n",
    "#    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's unfreeze the whole model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "# Let's build an optimizer\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"max\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# We need to recompile the model for these modifications to take effect\n",
    "es.set_model(model)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")  # , metrics=[\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history_2 = model.fit(\n",
    "    dataset, \n",
    "    callbacks=[es],\n",
    "    epochs=epochs_2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history_2, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_metric(history_2, \"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "valid_data = preprocess_data(X[valid_indices], fixed_text_to_square_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = model.predict(valid_data)\n",
    "print(roc_auc_score(Y[valid_indices], preds[:, 1]))\n",
    "del valid_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_data = preprocess_data(X[test_indices], fixed_text_to_square_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)\n",
    "print(roc_auc_score(Y[test_indices], preds[:, 1]))\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
