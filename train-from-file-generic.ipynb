{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2,\n",
    "    NASNetLarge,\n",
    "    NASNetMobile,\n",
    "    InceptionV3,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.applications import (\n",
    "    MobileNetV2,\n",
    "    NASNetLarge,\n",
    "    NASNetMobile,\n",
    "    InceptionV3,\n",
    ")\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    ZeroPadding2D,\n",
    "    Dense,\n",
    "    GlobalAveragePooling2D,\n",
    "    AveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from efficientnet.tfkeras import EfficientNetB0, EfficientNetB4, EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"santander-customer-satisfaction\"\n",
    "# IMAGE_SIZE = 640\n",
    "# dataset_name = \"springleaf-marketing-response\"\n",
    "# IMAGE_SIZE = 1408\n",
    "# dataset_name = \"segment\"\n",
    "# IMAGE_SIZE = 160\n",
    "# dataset_name = \"rl\"\n",
    "# IMAGE_SIZE = 160\n",
    "# dataset_name = \"open-payments\"\n",
    "# IMAGE_SIZE = 96\n",
    "# dataset_name = \"bank-marketing\"\n",
    "# IMAGE_SIZE = 160\n",
    "# dataset_name = \"springleaf-marketing-response\"\n",
    "# IMAGE_SIZE = 1408\n",
    "# dataset_name = \"bnp-cardif\"\n",
    "# IMAGE_SIZE = 384\n",
    "# dataset_name = \"albert\"\n",
    "# IMAGE_SIZE = 288\n",
    "dataset_name = \"open-payments\"\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "DATASET_FOLDER = Path(os.getcwd()) / f\"data/{dataset_name}\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "PREFETCH = 2\n",
    "\n",
    "\n",
    "ONE_CHANNEL = False\n",
    "\n",
    "# Target size for model to use\n",
    "TARGET_SIZE = 224\n",
    "PRETRAINED_MODEL = (\n",
    "    NASNetMobile  # EfficientNetB2  # EfficientNetB0  # MobileNetV2  # NASNetMobile\n",
    ")\n",
    "FROM_LAYER_RETRAIN = 0  # 119\n",
    "OUTPUT_LAYER = (\n",
    "    None  # -4  # Only for efficient Net, since include tops does not work None\n",
    ")\n",
    "\n",
    "epochs_1 = 30\n",
    "epochs_2 = 30\n",
    "patience = 2\n",
    "\n",
    "# Model name => image size, last_block retrain\n",
    "# NASNetMobile => 224,\n",
    "# NASNetLarge => 331,\n",
    "# MobileNetV2 => 96, ... 160..... 224, last layer => 128 (3 blocks), 137(2 blocks), 146(1 block)\n",
    "# InceptionV3 => 299,\n",
    "# Xception => 299,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T13:04:39.963383Z",
     "start_time": "2020-04-17T13:04:39.955914Z"
    }
   },
   "source": [
    "## Import + utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "from tensorflow_addons.activations import mish\n",
    "from concurrent.futures import ProcessPoolExecutor as PoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = None\n",
    "with (DATASET_FOLDER / \"prep_data\" / str(IMAGE_SIZE) / \"classnames.json\").open() as fp:\n",
    "    classnames = np.array(json.load(fp))\n",
    "OUTPUT_DIM = len(classnames)\n",
    "LOSS = \"binary_crossentropy\" if OUTPUT_DIM == 2 else \"categorical_crossentropy\"\n",
    "METRIC = \"AUC\" if OUTPUT_DIM == 2 else \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history, metric):\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f\"val_{metric}\"])\n",
    "    plt.title(f\"Model {metric}\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T13:09:05.896414Z",
     "start_time": "2020-04-17T13:09:05.860854Z"
    }
   },
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = None\n",
    "with (DATASET_FOLDER / \"prep_data\" / str(IMAGE_SIZE) / \"file_list.json\").open() as fp:\n",
    "    file_list = json.load(fp)\n",
    "file_list[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2] == classnames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    # img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    # return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_process_path(classnames):\n",
    "    def process_path(file_path):\n",
    "        label = tf.strings.split(file_path, os.path.sep)[-2] == classnames\n",
    "        # load the raw data from the file as a string\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.io.decode_compressed(img, \"GZIP\")\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        return img, label\n",
    "    return process_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_path = build_process_path(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def build_dataset(file_list, process_path, *, repeat, batch_size, prefetch):\n",
    "    dataset = file_list\n",
    "    if repeat:\n",
    "        dataset = file_list.repeat()\n",
    "        \n",
    "    dataset = file_list.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    if batch_size is not None:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    if prefetch is not None:\n",
    "        dataset = dataset.prefetch(prefetch)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = build_dataset(file_list[\"train\"], process_path,\n",
    "    repeat=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "                           prefetch=PREFETCH\n",
    "                          )\n",
    "dataset_valid = build_dataset(file_list[\"valid\"], process_path,\n",
    "    repeat=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "                           prefetch=PREFETCH\n",
    "                          )\n",
    "dataset_test = build_dataset(file_list[\"test\"], process_path,\n",
    "    repeat=False, \n",
    "    batch_size=BATCH_SIZE,\n",
    "                           prefetch=None\n",
    "                          )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_train = tf.data.Dataset.from_tensor_slices(file_list[\"train\"]).repeat()\n",
    "list_valid = tf.data.Dataset.from_tensor_slices(file_list[\"valid\"]).repeat()\n",
    "list_test = tf.data.Dataset.from_tensor_slices(file_list[\"test\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for path in list_train.take(1):\n",
    "    print(\"Path: \", path.numpy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_train = list_train.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "labeled_valid = list_valid.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "labeled_test = list_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_train = labeled_train.batch(BATCH_SIZE).prefetch(PREFETCH)\n",
    "dataset_valid = labeled_valid.batch(BATCH_SIZE).prefetch(PREFETCH)\n",
    "dataset_test = labeled_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataset_train.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, image_size, one_channel=False):\n",
    "    if one_channel:\n",
    "        imshow(image.reshape(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    else:\n",
    "        imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataset_train.take(1):\n",
    "    print(\"Label: \", label.numpy())\n",
    "    show_image(image.numpy(), IMAGE_SIZE, ONE_CHANNEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataset_valid.take(1):\n",
    "    print(\"Label: \", label.numpy())\n",
    "    show_image(image.numpy(), IMAGE_SIZE, ONE_CHANNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = np.ceil(len(file_list[\"train\"]) / BATCH_SIZE)\n",
    "steps_per_epoch_val = np.ceil(len(file_list[\"valid\"]) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model, using existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = mish\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_channel = 1 if ONE_CHANNEL else 3\n",
    "# create the base pre-trained model\n",
    "POOLING = int(np.ceil(IMAGE_SIZE / TARGET_SIZE))\n",
    "PADDING = np.floor(TARGET_SIZE - np.floor(IMAGE_SIZE / POOLING))\n",
    "PADDING_ASYM = int(PADDING % 2)\n",
    "\n",
    "PADDING = int(np.floor(PADDING / 2))\n",
    "\n",
    "inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, nb_channel))\n",
    "input_layer = inputs\n",
    "inputs = AveragePooling2D(pool_size=(POOLING, POOLING))(inputs)\n",
    "inputs = ZeroPadding2D(\n",
    "    padding=((PADDING, PADDING + PADDING_ASYM), (PADDING, PADDING + PADDING_ASYM),)\n",
    ")(inputs)\n",
    "\n",
    "base_model = PRETRAINED_MODEL(\n",
    "    input_tensor=inputs,\n",
    "    input_shape=(TARGET_SIZE, TARGET_SIZE, nb_channel),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = (\n",
    "    base_model.output\n",
    "    if OUTPUT_LAYER is None or OUTPUT_LAYER >= -1\n",
    "    else base_model.layers[-4].output\n",
    ")\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dense(512, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "x = Dense(128, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = Dense(512, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = Dense(256, activation=activation, kernel_initializer=\"he_normal\")(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(OUTPUT_DIM, activation=\"softmax\")(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# es.set_model(model)\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# We need to recompile the model for these modifications to take effect\n",
    "es.set_model(model)\n",
    "model.compile(optimizer=optimizer, loss=LOSS)  # , metrics=[METRIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "history_1 = model.fit(\n",
    "    dataset_train,\n",
    "    callbacks=[es],\n",
    "    epochs=epochs_1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    ")\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history_1, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_test = []\n",
    "for _, labels in dataset_test:\n",
    "    truth_test.append(np.argmax(labels, axis=1))\n",
    "truth_test = np.hstack(truth_test)\n",
    "truth_test\n",
    "\n",
    "truth_valid = []\n",
    "for i, (_, labels) in enumerate(dataset_valid):\n",
    "    truth_valid.append(np.argmax(labels, axis=1))\n",
    "    if i >= steps_per_epoch_val - 1:\n",
    "        break\n",
    "truth_valid = np.hstack(truth_valid)\n",
    "truth_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model.predict(dataset_valid, steps=steps_per_epoch_val)\n",
    "preds_valid.shape\n",
    "if OUTPUT_DIM > 2:\n",
    "    print(\n",
    "        f\"Accuracy valid: {accuracy_score(truth_valid, np.argmax(preds_valid, axis=1))}\"\n",
    "    )\n",
    "if OUTPUT_DIM == 2:\n",
    "    print(f\"ROC AUC valid: {roc_auc_score(truth_valid, preds_valid[:, 1])}\")\n",
    "preds_test = model.predict(dataset_test)\n",
    "preds_test.shape\n",
    "if OUTPUT_DIM > 2:\n",
    "    print(f\"Accuracy test: {accuracy_score(truth_test, np.argmax(preds_test, axis=1))}\")\n",
    "if OUTPUT_DIM == 2:\n",
    "    print(f\"ROC AUC test: {roc_auc_score(truth_test, preds_test[:, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metric(history_1, METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze and fit more/all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's unfreeze the whole model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[FROM_LAYER_RETRAIN:]:  # [144:]:  # [1019:]:  # [293:]:\n",
    "    layer.trainable = True\n",
    "# Let's build an optimizer\n",
    "optimizer = Lookahead(RectifiedAdam(), sync_period=6, slow_step_size=0.5)\n",
    "# optimizer=SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# We need to recompile the model for these modifications to take effect\n",
    "es.set_model(model)\n",
    "model.compile(optimizer=optimizer, loss=LOSS)  # , metrics=[METRIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history_2 = model.fit(\n",
    "    dataset_train,\n",
    "    callbacks=[es],\n",
    "    epochs=epochs_2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_valid,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history_2, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_metric(history_2, METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T13:16:20.349217Z",
     "start_time": "2020-04-17T13:16:16.612Z"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model.predict(dataset_valid, steps=steps_per_epoch_val)\n",
    "preds_valid.shape\n",
    "if OUTPUT_DIM > 2:\n",
    "    print(\n",
    "        f\"Accuracy valid: {accuracy_score(truth_valid, np.argmax(preds_valid, axis=1))}\"\n",
    "    )\n",
    "if OUTPUT_DIM == 2:\n",
    "    print(f\"ROC AUC valid: {roc_auc_score(truth_valid, preds_valid[:, 1])}\")\n",
    "preds_test = model.predict(dataset_test)\n",
    "preds_test.shape\n",
    "if OUTPUT_DIM > 2:\n",
    "    print(f\"Accuracy test: {accuracy_score(truth_test, np.argmax(preds_test, axis=1))}\")\n",
    "if OUTPUT_DIM == 2:\n",
    "    print(f\"ROC AUC test: {roc_auc_score(truth_test, preds_test[:, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0\n",
    "\n",
    "# EfficientNetB2 2h => ROC AUC valid: 0.9357130861335854 ROC AUC test: 0.9330937133279599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"santander-customer-satisfaction\"\n",
    "# 0.8164865598696714 => target size 96, whole re train\n",
    "# 0.8141824599511267 => target size 224, whole re train\n",
    "# ROC AUC valid: 0.8268400760249797 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "\n",
    "# give me some credit\n",
    "# 0.8462134942186483 => target size 160, whole train (batch 128, 2layers 1024, 128)\n",
    "\n",
    "# ROC AUC valid: 0.8348917439829162\n",
    "# RL\n",
    "# ROC AUC valid: 0.892118469133795 => 160, whole, batch 64, 1layer 1024\n",
    "# ROC AUC valid: 0.9233534348199217 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "# ROC AUC valid: 0.9465346534653467 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "\n",
    "# Open payment\n",
    "# ROC AUC valid: 0.9186458210299415 => 96 =>160, whole, batch 64, 1layer 1024\n",
    "# ROC AUC valid: 0.8778868370932499 => 96 => 96, whole, batch 64, 1layer 1024\n",
    "# ROC AUC valid: 0.8917381493730192 => 96 => 224, whole, batch 64, 1layer 1024\n",
    "# ROC AUC valid: 0.9090374872044725 => 96 => 96, whole, batch 64, 1layer 1024 -> 512 -> 128\n",
    "# ROC AUC valid: 0.886417393797122 => 96 => 160, whole, batch 64, 1layer 1024 -> 512 -> 128\n",
    "# ROC AUC valid: 0.9045819676568436 => 96 => 96, whole, batch 64, 1layer 1024 -> 128, no dropout\n",
    "# ROC AUC valid: 0.9045819676568436 => 96 => 96, whole, batch 64, 1layer 1024 -> 128, no dropout\n",
    "# ROC AUC valid: 0.9406275221953189 => 96 => 96, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "# ROC AUC valid: 0.9399110034154216 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "# ROC AUC valid: 0.8811034128677376 => 96 => 96, whole, batch 128, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "\n",
    "# Bank marketing\n",
    "# ROC AUC valid: 0.7970734141661526 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "\n",
    "# Albert\n",
    "# ROC AUC valid: 0.7500980687987842 => 288 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "\n",
    "# bnp-cardif\n",
    "# ROC AUC valid: 0.7206667869818926 => 384 => 96, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"santander-customer-satisfaction\"\n",
    "# 0.833417731838137 => target size 96, whole train\n",
    "# 0.8170226029745679 => target size 224, whole train\n",
    "# ROC AUC test: 0.8348549041045967 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "\n",
    "# give me some credit\n",
    "# 0.8453497574694486 => target size 160, whole train (batch 128, 2layers 1024, 128)\n",
    "# ROC AUC test: 0.8447058873195916\n",
    "\n",
    "# RL\n",
    "# ROC AUC test: 0.9051288159651395 => 160, whole, batch 64, 1layer 1024\n",
    "# ROC AUC test: 0.9128674518211912 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "# ROC AUC test: 0.9456874816987527 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "# ROC AUC test: 0.9399110034154216 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout, earlystoping eevry time\n",
    "\n",
    "# Open payment\n",
    "# ROC AUC test: 0.916907759155486 => 96 =>160, whole, batch 64, 1layer 1024\n",
    "# ROC AUC test: 0.8931101859362467 => 96 => 96, whole, batch 64, 1layer 1024\n",
    "# ROC AUC test: 0.8938387451368033 => 96 => 224, whole, batch 64, 1layer 1024\n",
    "# ROC AUC test: 0.9067445823812874 => 96 => 96, whole, batch 64, 1layer 1024 -> 512 -> 128\n",
    "# ROC AUC test: 0.8954549557710788 => 96 => 160, whole, batch 64, 1layer 1024 -> 512 -> 128\n",
    "# ROC AUC test: 0.9062895529860363 => 96 => 96, whole, batch 64, 1layer 1024 -> 128, no dropout\n",
    "# ROC AUC test: 0.9403517762951931 => 96 => 96, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "\n",
    "# Bank marketing\n",
    "# ROC AUC valid: 0.7959000291791145 => 160 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "\n",
    "# Albert\n",
    "# ROC AUC test: 0.7487660412524685 => 288 => 160, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n",
    "\n",
    "# bnp-cardif\n",
    "# ROC AUC test: 0.725934546476426 => 384 => 96, whole, batch 32, 1layer 1024 -> 512 -> 128, no dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T18:52:58.124042Z",
     "start_time": "2020-04-19T18:50:57.222Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T18:52:58.125007Z",
     "start_time": "2020-04-19T18:50:57.224Z"
    },
    "hidden": true
   },
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T18:52:58.125913Z",
     "start_time": "2020-04-19T18:50:57.225Z"
    },
    "hidden": true
   },
   "source": [
    "# import keras\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "# from matplotlib.colors import LinearSegmentedColormap\n",
    "# import numpy as np\n",
    "import shap\n",
    "\n",
    "# import keras.backend as K\n",
    "# import json\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "# utility function to visualize SHAP values in larger image formats\n",
    "# this modifies the `shap.image_plot(...)` function\n",
    "def visualize_model_decisions(shap_values, x, labels=None, figsize=(20, 30)):\n",
    "\n",
    "    colors = []\n",
    "    for l in np.linspace(1, 0, 100):\n",
    "        colors.append((30.0 / 255, 136.0 / 255, 229.0 / 255, l))\n",
    "    for l in np.linspace(0, 1, 100):\n",
    "        colors.append((255.0 / 255, 13.0 / 255, 87.0 / 255, l))\n",
    "    red_transparent_blue = LinearSegmentedColormap.from_list(\n",
    "        \"red_transparent_blue\", colors\n",
    "    )\n",
    "\n",
    "    multi_output = True\n",
    "    if type(shap_values) != list:\n",
    "        multi_output = False\n",
    "        shap_values = [shap_values]\n",
    "\n",
    "    # make sure labels\n",
    "    if labels is not None:\n",
    "        assert (\n",
    "            labels.shape[0] == shap_values[0].shape[0]\n",
    "        ), \"Labels must have same row count as shap_values arrays!\"\n",
    "        if multi_output:\n",
    "            assert labels.shape[1] == len(\n",
    "                shap_values\n",
    "            ), \"Labels must have a column for each output in shap_values!\"\n",
    "        else:\n",
    "            assert (\n",
    "                len(labels.shape) == 1\n",
    "            ), \"Labels must be a vector for single output shap_values.\"\n",
    "\n",
    "    # plot our explanations\n",
    "    fig_size = figsize\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=x.shape[0], ncols=len(shap_values) + 1, figsize=fig_size\n",
    "    )\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = axes.reshape(1, axes.size)\n",
    "    for row in range(x.shape[0]):\n",
    "        x_curr = x[row].copy()\n",
    "\n",
    "        # make sure\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 1:\n",
    "            x_curr = x_curr.reshape(x_curr.shape[:2])\n",
    "        if x_curr.max() > 1:\n",
    "            x_curr /= 255.0\n",
    "\n",
    "        axes[row, 0].imshow(x_curr)\n",
    "        axes[row, 0].axis(\"off\")\n",
    "\n",
    "        # get a grayscale version of the image\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 3:\n",
    "            x_curr_gray = (\n",
    "                0.2989 * x_curr[:, :, 0]\n",
    "                + 0.5870 * x_curr[:, :, 1]\n",
    "                + 0.1140 * x_curr[:, :, 2]\n",
    "            )  # rgb to gray\n",
    "        else:\n",
    "            x_curr_gray = x_curr\n",
    "\n",
    "        if len(shap_values[0][row].shape) == 2:\n",
    "            abs_vals = np.stack(\n",
    "                [np.abs(shap_values[i]) for i in range(len(shap_values))], 0\n",
    "            ).flatten()\n",
    "        else:\n",
    "            abs_vals = np.stack(\n",
    "                [np.abs(shap_values[i].sum(-1)) for i in range(len(shap_values))], 0\n",
    "            ).flatten()\n",
    "        max_val = np.nanpercentile(abs_vals, 99.9)\n",
    "        for i in range(len(shap_values)):\n",
    "            if labels is not None:\n",
    "                axes[row, i + 1].set_title(labels[row, i])\n",
    "            sv = (\n",
    "                shap_values[i][row]\n",
    "                if len(shap_values[i][row].shape) == 2\n",
    "                else shap_values[i][row].sum(-1)\n",
    "            )\n",
    "            axes[row, i + 1].imshow(\n",
    "                x_curr_gray,\n",
    "                cmap=plt.get_cmap(\"gray\"),\n",
    "                alpha=0.15,\n",
    "                extent=(-1, sv.shape[0], sv.shape[1], -1),\n",
    "            )\n",
    "            im = axes[row, i + 1].imshow(\n",
    "                sv, cmap=red_transparent_blue, vmin=-max_val, vmax=max_val\n",
    "            )\n",
    "            axes[row, i + 1].axis(\"off\")\n",
    "\n",
    "    cb = fig.colorbar(\n",
    "        im,\n",
    "        ax=np.ravel(axes).tolist(),\n",
    "        label=\"SHAP value\",\n",
    "        orientation=\"horizontal\",\n",
    "        aspect=fig_size[0] / 0.2,\n",
    "    )\n",
    "    cb.outline.set_visible(False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T18:52:58.126811Z",
     "start_time": "2020-04-19T18:50:57.227Z"
    },
    "hidden": true
   },
   "source": [
    "# make model predictions\n",
    "n_layer = 311\n",
    "\n",
    "\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [x]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[n_layer].input, model.layers[-1].output),\n",
    "    map2layer(preprocess_input(X.copy()), n_layer),\n",
    ")\n",
    "shap_values, indexes = e.shap_values(map2layer(to_predict, n_layer), ranked_outputs=2)\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "print(index_names)\n",
    "# visualize model decisions\n",
    "visualize_model_decisions(\n",
    "    shap_values=shap_values, x=to_predict, labels=index_names, figsize=(20, 40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
